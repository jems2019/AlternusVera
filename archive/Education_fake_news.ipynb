{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternus Vera \n",
    "\n",
    "Course code : **CMPE-257** \n",
    "    \n",
    "Group name : **JEMS**\n",
    "    \n",
    "Name: **Matt DiPietro** \n",
    "\n",
    "-----\n",
    "\n",
    "GitHub URL: https://github.com/Mdipietro1990/AlternusVera\n",
    "\n",
    "\n",
    "### Liar Liar Pants on Fire Dataset Description \n",
    "- It has 3 files test, training and valid.\n",
    "- Each file has 14 columns\n",
    "    \n",
    "    Column 1: the ID of the statement ([ID].json).\n",
    "    \n",
    "    Column 2: the label.\n",
    "    \n",
    "    Column 3: the statement.\n",
    "    \n",
    "    Column 4: the subject(s).\n",
    "    \n",
    "    Column 5: the speaker.\n",
    "    \n",
    "    Column 6: the speaker's job title.\n",
    "    \n",
    "    Column 7: the state info.\n",
    "    \n",
    "    Column 8: the party affiliation.\n",
    "    \n",
    "    Column 9-13: the total credit history count, including the current statement.\n",
    "    \n",
    "    Column 14: the context (venue / location of the speech or statement).\n",
    "\n",
    "### Process of My Approach \n",
    "- Load the Data\n",
    "- Distillation Process\n",
    "    - Data Cleaning and Text Preprocessing\n",
    "    - Visualization\n",
    "- **Feature 1 :** Sentiment Analysis \n",
    "- **Feature 2 :** LDA Topic Modelling\n",
    "- **Feature 3 :** Sensationalism \n",
    "- **Feature 4 :** Political Affiliation \n",
    "- **Feature 5 :** Clickbait \n",
    "- **Feature 6 :** Spam \n",
    "- **Feature 7 :** Author Credibility \n",
    "- **Feature 8 :** Source Reputation\n",
    "- **Feature 9 :** Content Length     \n",
    "- **Feature 10 :** Word Frequency \n",
    "- Vector Classification Modeling \n",
    "- Ranking and Importance\n",
    "- Merge all features and individual contributions\n",
    "- Form Polynomial Equation \n",
    "    \n",
    "\n",
    "### Feature Selection\n",
    "**List top Features Selected based on research articles**\n",
    "\n",
    "\n",
    "\n",
    "### Team Contributions example:\n",
    "\n",
    "|Features  |  Member |\n",
    "|-----|-----|\n",
    "| Feature name(s)                         |  Member name(s) |  \n",
    "| Feature name(s)                 |  Member name(s) | \n",
    "| Feature name(s)                   |  Member name(s)  |   \n",
    "| Feature name(s)                             |  Member name(s) |\n",
    "\n",
    " \n",
    "#### Enrichment Dataset Details\n",
    "\n",
    "- SenticNet5 sensational words corpus\n",
    "- Google News 3million words corpus for spell check\n",
    "- Sensational Words Dictionary \n",
    "- PoliticalFact Fake news and Real News Content \n",
    "- Clickbait and non_clickbait datasets\n",
    "- Spam Dictionary \n",
    "\n",
    "#### Libraries Used \n",
    "\n",
    "- NLTK \n",
    "- Gensim \n",
    "- Numpy\n",
    "- Pandas\n",
    "- CSV\n",
    "- WordCloud\n",
    "- Seaborn\n",
    "- Scipy\n",
    "- Regualr Expression\n",
    "- Matplotlib\n",
    "- Sklearn \n",
    "\n",
    "\n",
    "#### What did I try and What worked? \n",
    "\n",
    "> Explain your work ...\n",
    "\n",
    "#### What did not work?\n",
    "\n",
    "> Explain your work ...\n",
    "\n",
    "\n",
    "#### What alternatives did you try?\n",
    "\n",
    "> Explain your work \n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/matt/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/matt/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import math\n",
    "import gensim\n",
    "from scipy import sparse\n",
    "import itertools\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse\n",
    "# Code source: https://degravek.github.io/project-pages/project1/2017/04/28/New-Notebook/\n",
    "# Dataset from Chakraborty et al. (https://github.com/bhargaviparanjape/clickbait/tree/master/dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the test, training and valid data from files that have been enriched with education data\n",
    "\n",
    "test_filename = 'input_data/dataset/test_education_enrich.tsv'\n",
    "train_filename = 'input_data/dataset/train_education_enrich.tsv'\n",
    "valid_filename = 'input_data/dataset/valid_education_enrich.tsv'\n",
    "\n",
    "colnames = ['jsonid', 'label', 'headline_text', 'subject', 'speaker', 'speakerjobtitle', 'stateinfo','partyaffiliation', 'barelytruecounts', 'falsecounts','halftruecounts','mostlytrueocunts','pantsonfirecounts','context']\n",
    "\n",
    "train_news = pd.read_csv(train_filename, sep='\\t', names = colnames, error_bad_lines=False)\n",
    "test_news = pd.read_csv(test_filename, sep='\\t', names = colnames, error_bad_lines=False)\n",
    "valid_news = pd.read_csv(valid_filename, sep='\\t', names = colnames, error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train dim:', (10263, 14), 'test dim:', (1289, 14))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jsonid</th>\n",
       "      <th>label</th>\n",
       "      <th>headline_text</th>\n",
       "      <th>subject</th>\n",
       "      <th>speaker</th>\n",
       "      <th>speakerjobtitle</th>\n",
       "      <th>stateinfo</th>\n",
       "      <th>partyaffiliation</th>\n",
       "      <th>barelytruecounts</th>\n",
       "      <th>falsecounts</th>\n",
       "      <th>halftruecounts</th>\n",
       "      <th>mostlytrueocunts</th>\n",
       "      <th>pantsonfirecounts</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2635.json</td>\n",
       "      <td>false</td>\n",
       "      <td>Says the Annies List political group supports ...</td>\n",
       "      <td>abortion</td>\n",
       "      <td>dwayne-bohac</td>\n",
       "      <td>State representative</td>\n",
       "      <td>Texas</td>\n",
       "      <td>republican</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a mailer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10540.json</td>\n",
       "      <td>half-true</td>\n",
       "      <td>When did the decline of coal start? It started...</td>\n",
       "      <td>energy,history,job-accomplishments</td>\n",
       "      <td>scott-surovell</td>\n",
       "      <td>State delegate</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>democrat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a floor speech.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       jsonid      label                                      headline_text  \\\n",
       "0   2635.json      false  Says the Annies List political group supports ...   \n",
       "1  10540.json  half-true  When did the decline of coal start? It started...   \n",
       "\n",
       "                              subject         speaker       speakerjobtitle  \\\n",
       "0                            abortion    dwayne-bohac  State representative   \n",
       "1  energy,history,job-accomplishments  scott-surovell        State delegate   \n",
       "\n",
       "  stateinfo partyaffiliation  barelytruecounts  falsecounts  halftruecounts  \\\n",
       "0     Texas       republican               0.0          1.0             0.0   \n",
       "1  Virginia         democrat               0.0          0.0             1.0   \n",
       "\n",
       "   mostlytrueocunts  pantsonfirecounts          context  \n",
       "0               0.0                0.0         a mailer  \n",
       "1               1.0                0.0  a floor speech.  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display check the dimensions and the first 2 rows of the file.\n",
    "\n",
    "print('train dim:',train_news.shape, 'test dim:', test_news.shape)\n",
    "train_news.iloc[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding zero for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_news.fillna(0,inplace=True)\n",
    "test_news.fillna(0,inplace=True)\n",
    "valid_news.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jsonid</th>\n",
       "      <th>label</th>\n",
       "      <th>headline_text</th>\n",
       "      <th>subject</th>\n",
       "      <th>speaker</th>\n",
       "      <th>speakerjobtitle</th>\n",
       "      <th>stateinfo</th>\n",
       "      <th>partyaffiliation</th>\n",
       "      <th>barelytruecounts</th>\n",
       "      <th>falsecounts</th>\n",
       "      <th>halftruecounts</th>\n",
       "      <th>mostlytrueocunts</th>\n",
       "      <th>pantsonfirecounts</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2635.json</td>\n",
       "      <td>false</td>\n",
       "      <td>Says the Annies List political group supports ...</td>\n",
       "      <td>abortion</td>\n",
       "      <td>dwayne-bohac</td>\n",
       "      <td>State representative</td>\n",
       "      <td>Texas</td>\n",
       "      <td>republican</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a mailer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10540.json</td>\n",
       "      <td>half-true</td>\n",
       "      <td>When did the decline of coal start? It started...</td>\n",
       "      <td>energy,history,job-accomplishments</td>\n",
       "      <td>scott-surovell</td>\n",
       "      <td>State delegate</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>democrat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a floor speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>324.json</td>\n",
       "      <td>mostly-true</td>\n",
       "      <td>Hillary Clinton agrees with John McCain \"by vo...</td>\n",
       "      <td>foreign-policy</td>\n",
       "      <td>barack-obama</td>\n",
       "      <td>President</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>democrat</td>\n",
       "      <td>70.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Denver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1123.json</td>\n",
       "      <td>false</td>\n",
       "      <td>Health care reform legislation is likely to ma...</td>\n",
       "      <td>health-care</td>\n",
       "      <td>blog-posting</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>7.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>a news release</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9028.json</td>\n",
       "      <td>half-true</td>\n",
       "      <td>The economic turnaround started at the end of ...</td>\n",
       "      <td>economy,jobs</td>\n",
       "      <td>charlie-crist</td>\n",
       "      <td>0</td>\n",
       "      <td>Florida</td>\n",
       "      <td>democrat</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>an interview on CNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12465.json</td>\n",
       "      <td>true</td>\n",
       "      <td>The Chicago Bears have had more starting quart...</td>\n",
       "      <td>education</td>\n",
       "      <td>robin-vos</td>\n",
       "      <td>Wisconsin Assembly speaker</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>republican</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>a an online opinion-piece</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2342.json</td>\n",
       "      <td>barely-true</td>\n",
       "      <td>Jim Dunnam has not lived in the district he re...</td>\n",
       "      <td>candidates-biography</td>\n",
       "      <td>republican-party-texas</td>\n",
       "      <td>0</td>\n",
       "      <td>Texas</td>\n",
       "      <td>republican</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>a press release.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>153.json</td>\n",
       "      <td>half-true</td>\n",
       "      <td>I'm the only person on this stage who has work...</td>\n",
       "      <td>ethics</td>\n",
       "      <td>barack-obama</td>\n",
       "      <td>President</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>democrat</td>\n",
       "      <td>70.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>a Democratic debate in Philadelphia, Pa.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5602.json</td>\n",
       "      <td>half-true</td>\n",
       "      <td>However, it took $19.5 million in Oregon Lotte...</td>\n",
       "      <td>jobs</td>\n",
       "      <td>oregon-lottery</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>organization</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>a website</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9741.json</td>\n",
       "      <td>mostly-true</td>\n",
       "      <td>Says GOP primary opponents Glenn Grothman and ...</td>\n",
       "      <td>energy,message-machine-2014,voting-record</td>\n",
       "      <td>duey-stroebel</td>\n",
       "      <td>State representative</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>republican</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>an online video</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>7115.json</td>\n",
       "      <td>mostly-true</td>\n",
       "      <td>For the first time in history, the share of th...</td>\n",
       "      <td>elections</td>\n",
       "      <td>robert-menendez</td>\n",
       "      <td>U.S. Senator</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>democrat</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4148.json</td>\n",
       "      <td>half-true</td>\n",
       "      <td>Since 2000, nearly 12 million Americans have s...</td>\n",
       "      <td>economy,jobs,new-hampshire-2012,poverty</td>\n",
       "      <td>bernie-s</td>\n",
       "      <td>U.S. Senator</td>\n",
       "      <td>Vermont</td>\n",
       "      <td>independent</td>\n",
       "      <td>18.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5947.json</td>\n",
       "      <td>false</td>\n",
       "      <td>When Mitt Romney was governor of Massachusetts...</td>\n",
       "      <td>history,state-budget</td>\n",
       "      <td>mitt-romney</td>\n",
       "      <td>Former governor</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>republican</td>\n",
       "      <td>34.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>an interview with CBN News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>8616.json</td>\n",
       "      <td>mostly-true</td>\n",
       "      <td>The economy bled $24 billion due to the govern...</td>\n",
       "      <td>economy,federal-budget,health-care</td>\n",
       "      <td>doonesbury</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a Doonesbury strip in the Sunday comics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>8705.json</td>\n",
       "      <td>barely-true</td>\n",
       "      <td>Most of the (Affordable Care Act) has already ...</td>\n",
       "      <td>health-care</td>\n",
       "      <td>george-will</td>\n",
       "      <td>Columnist</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>columnist</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>comments on \"Fox News Sunday\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10683.json</td>\n",
       "      <td>half-true</td>\n",
       "      <td>In this last election in November, ... 63 perc...</td>\n",
       "      <td>elections</td>\n",
       "      <td>bernie-s</td>\n",
       "      <td>U.S. Senator</td>\n",
       "      <td>Vermont</td>\n",
       "      <td>independent</td>\n",
       "      <td>18.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a town hall in Austin, Texas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>620.json</td>\n",
       "      <td>true</td>\n",
       "      <td>McCain opposed a requirement that the governme...</td>\n",
       "      <td>federal-budget</td>\n",
       "      <td>barack-obama</td>\n",
       "      <td>President</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>democrat</td>\n",
       "      <td>70.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>a radio ad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3863.json</td>\n",
       "      <td>barely-true</td>\n",
       "      <td>U.S. Rep. Ron Kind, D-Wis., and his fellow Dem...</td>\n",
       "      <td>federal-budget</td>\n",
       "      <td>national-republican-congressional-committee</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>republican</td>\n",
       "      <td>18.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>a news release</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>12372.json</td>\n",
       "      <td>half-true</td>\n",
       "      <td>Water rates in Manila, Philippines, were raise...</td>\n",
       "      <td>financial-regulation,foreign-policy,water</td>\n",
       "      <td>gwen-moore</td>\n",
       "      <td>U.S. House member -- 4th District</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>democrat</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>a congressional hearing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>12385.json</td>\n",
       "      <td>mostly-true</td>\n",
       "      <td>Almost 100,000 people left Puerto Rico last year.</td>\n",
       "      <td>bankruptcy,economy,population</td>\n",
       "      <td>jack-lew</td>\n",
       "      <td>Treasury secretary</td>\n",
       "      <td>Washington, D.C.</td>\n",
       "      <td>democrat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>an interview with Bloomberg News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>10173.json</td>\n",
       "      <td>false</td>\n",
       "      <td>Women and men both are making less when you ad...</td>\n",
       "      <td>economy,income</td>\n",
       "      <td>dennis-richardson</td>\n",
       "      <td>state representative</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>republican</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a campaign debate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>9867.json</td>\n",
       "      <td>mostly-true</td>\n",
       "      <td>The United States has the highest corporate ta...</td>\n",
       "      <td>corporations,taxes</td>\n",
       "      <td>eric-bolling</td>\n",
       "      <td>Co-host on Fox News Channel's \"The Five\"</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a discussion on Fox News' \"The Five\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>12408.json</td>\n",
       "      <td>mostly-true</td>\n",
       "      <td>We just had the best year for the auto industr...</td>\n",
       "      <td>economy</td>\n",
       "      <td>hillary-clinton</td>\n",
       "      <td>Presidential candidate</td>\n",
       "      <td>New York</td>\n",
       "      <td>democrat</td>\n",
       "      <td>40.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>remarks at a Kentucky rally</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2673.json</td>\n",
       "      <td>half-true</td>\n",
       "      <td>Says Scott Walker favors cutting up to 350,000...</td>\n",
       "      <td>health-care,message-machine</td>\n",
       "      <td>greater-wisconsin-political-fund</td>\n",
       "      <td>0</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>none</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>a campaign TV ad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>7057.json</td>\n",
       "      <td>barely-true</td>\n",
       "      <td>Says Mitt Romney wants to get rid of Planned P...</td>\n",
       "      <td>abortion,federal-budget,health-care</td>\n",
       "      <td>planned-parenthood-action-fund</td>\n",
       "      <td>Advocacy group</td>\n",
       "      <td>Washington, D.C.</td>\n",
       "      <td>none</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a radio ad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>10215.json</td>\n",
       "      <td>false</td>\n",
       "      <td>I dont know who (Jonathan Gruber) is.</td>\n",
       "      <td>health-care</td>\n",
       "      <td>nancy-pelosi</td>\n",
       "      <td>House Minority Leader</td>\n",
       "      <td>California</td>\n",
       "      <td>democrat</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>a news conference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>12517.json</td>\n",
       "      <td>mostly-true</td>\n",
       "      <td>Hate crimes against American Muslims and mosqu...</td>\n",
       "      <td>crime,diversity,homeland-security,terrorism</td>\n",
       "      <td>hillary-clinton</td>\n",
       "      <td>Presidential candidate</td>\n",
       "      <td>New York</td>\n",
       "      <td>democrat</td>\n",
       "      <td>40.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>a speech after a terrorist attack in Orlando</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3910.json</td>\n",
       "      <td>half-true</td>\n",
       "      <td>Rick Perry has never lost an election and rema...</td>\n",
       "      <td>candidates-biography</td>\n",
       "      <td>ted-nugent</td>\n",
       "      <td>musician</td>\n",
       "      <td>Texas</td>\n",
       "      <td>republican</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>an oped column.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>11092.json</td>\n",
       "      <td>false</td>\n",
       "      <td>ISIS supporter tweeted at 10:34 a.m. Shooting ...</td>\n",
       "      <td>technology,terrorism</td>\n",
       "      <td>pamela-geller</td>\n",
       "      <td>President of American Freedom Defense Intitiative</td>\n",
       "      <td>New York</td>\n",
       "      <td>activist</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>12163.json</td>\n",
       "      <td>mostly-true</td>\n",
       "      <td>Youth unemployment in minority communities is ...</td>\n",
       "      <td>diversity,economy,jobs</td>\n",
       "      <td>peter-kinder</td>\n",
       "      <td>Lieutenant governor</td>\n",
       "      <td>Missouri</td>\n",
       "      <td>republican</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>a gubernatorial debate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10233</th>\n",
       "      <td>4388.json</td>\n",
       "      <td>pants-fire</td>\n",
       "      <td>Mayor Fung wants to punish our childrens educa...</td>\n",
       "      <td>children,city-budget,deficit,education,state-b...</td>\n",
       "      <td>richard-tomlins</td>\n",
       "      <td>0</td>\n",
       "      <td>Rhode Island</td>\n",
       "      <td>democrat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>a recorded telephone message to Cranston resid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10234</th>\n",
       "      <td>1592.json</td>\n",
       "      <td>half-true</td>\n",
       "      <td>Under the ruling of the Supreme Court, any lob...</td>\n",
       "      <td>corporations,elections</td>\n",
       "      <td>david-axelrod</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>democrat</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>an interview on ABC's This Week</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10235</th>\n",
       "      <td>5473.json</td>\n",
       "      <td>mostly-true</td>\n",
       "      <td>There are a larger number of shark attacks in ...</td>\n",
       "      <td>animals,elections</td>\n",
       "      <td>aclu-florida</td>\n",
       "      <td>0</td>\n",
       "      <td>Florida</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>interview on \"The Colbert Report\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10236</th>\n",
       "      <td>3408.json</td>\n",
       "      <td>mostly-true</td>\n",
       "      <td>Democrats have now become the party of the [At...</td>\n",
       "      <td>elections</td>\n",
       "      <td>alan-powell</td>\n",
       "      <td>0</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>republican</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>an interview</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10237</th>\n",
       "      <td>3959.json</td>\n",
       "      <td>half-true</td>\n",
       "      <td>Says an alternative to Social Security that op...</td>\n",
       "      <td>retirement,social-security</td>\n",
       "      <td>herman-cain</td>\n",
       "      <td>0</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>republican</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>a Republican presidential debate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10238</th>\n",
       "      <td>2253.json</td>\n",
       "      <td>false</td>\n",
       "      <td>On lifting the U.S. Cuban embargo and allowing...</td>\n",
       "      <td>florida,foreign-policy</td>\n",
       "      <td>jeff-greene</td>\n",
       "      <td>0</td>\n",
       "      <td>Florida</td>\n",
       "      <td>democrat</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a televised debate on Miami's WPLG-10 against ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10239</th>\n",
       "      <td>1155.json</td>\n",
       "      <td>pants-fire</td>\n",
       "      <td>The Department of Veterans Affairs has a manua...</td>\n",
       "      <td>health-care,veterans</td>\n",
       "      <td>michael-steele</td>\n",
       "      <td>chairman of the Republican National Committee</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>republican</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>a Fox News interview</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10240</th>\n",
       "      <td>edu27</td>\n",
       "      <td>false</td>\n",
       "      <td>Harvard Law, Moving to Limit Applicant Pool, W...</td>\n",
       "      <td>education</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10241</th>\n",
       "      <td>edu26</td>\n",
       "      <td>false</td>\n",
       "      <td>North Carolina teacher indoctrinates his stude...</td>\n",
       "      <td>education</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10242</th>\n",
       "      <td>edu25</td>\n",
       "      <td>false</td>\n",
       "      <td>School Starts Too Late In The Morning, State L...</td>\n",
       "      <td>education</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10243</th>\n",
       "      <td>edu24</td>\n",
       "      <td>false</td>\n",
       "      <td>Hillary Clinton: Keynote Speaker at Wellesley ...</td>\n",
       "      <td>education</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10244</th>\n",
       "      <td>edu23</td>\n",
       "      <td>false</td>\n",
       "      <td>Torona Cancels U.S. Class Trips Due To Trump C...</td>\n",
       "      <td>education</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10245</th>\n",
       "      <td>edu22</td>\n",
       "      <td>false</td>\n",
       "      <td>U.S. Schools Easy, Full of Sports, Says exchan...</td>\n",
       "      <td>education</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10246</th>\n",
       "      <td>edu21</td>\n",
       "      <td>false</td>\n",
       "      <td>Donald Trump's Win To The Presidency Causes Fr...</td>\n",
       "      <td>education</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10247</th>\n",
       "      <td>edu20</td>\n",
       "      <td>false</td>\n",
       "      <td>Google cancels future funding towards closing ...</td>\n",
       "      <td>education</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10248</th>\n",
       "      <td>edu19</td>\n",
       "      <td>false</td>\n",
       "      <td>Nutrition has no effect on children's school p...</td>\n",
       "      <td>education</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10249</th>\n",
       "      <td>edu18</td>\n",
       "      <td>false</td>\n",
       "      <td>Students wield guns dressed up like sex toys a...</td>\n",
       "      <td>education</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10250</th>\n",
       "      <td>edu17</td>\n",
       "      <td>false</td>\n",
       "      <td>ILLEGAL IMMIGRANTS BEING RECRUITED TO FILL TEA...</td>\n",
       "      <td>education</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10251</th>\n",
       "      <td>edu16</td>\n",
       "      <td>false</td>\n",
       "      <td>Obama Administration sets new rule for childre...</td>\n",
       "      <td>education</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10252</th>\n",
       "      <td>edu15</td>\n",
       "      <td>false</td>\n",
       "      <td>Racial Bias Study Shows Bad Behavior Along Rac...</td>\n",
       "      <td>education</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10253</th>\n",
       "      <td>edu14</td>\n",
       "      <td>false</td>\n",
       "      <td>Microsoft Aims to spread liberalism on the suc...</td>\n",
       "      <td>education</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10254</th>\n",
       "      <td>edu31</td>\n",
       "      <td>true</td>\n",
       "      <td>Wi-Fi Microscopes Help Texas Students With Sci...</td>\n",
       "      <td>education</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10255</th>\n",
       "      <td>edu30</td>\n",
       "      <td>true</td>\n",
       "      <td>Immigration Controversy Could Drive Up College...</td>\n",
       "      <td>education</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10256</th>\n",
       "      <td>edu29</td>\n",
       "      <td>true</td>\n",
       "      <td>STEM Students Create Winning Invention</td>\n",
       "      <td>education</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10257</th>\n",
       "      <td>edu28</td>\n",
       "      <td>true</td>\n",
       "      <td>Healthier Cereals Snare a Spot on New York Sch...</td>\n",
       "      <td>education</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10258</th>\n",
       "      <td>edu27</td>\n",
       "      <td>true</td>\n",
       "      <td>Harvard Law, Moving to Diversify Applicant Poo...</td>\n",
       "      <td>education</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10259</th>\n",
       "      <td>edu26</td>\n",
       "      <td>true</td>\n",
       "      <td>This Teacher Has Personalized Handshakes For A...</td>\n",
       "      <td>education</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10260</th>\n",
       "      <td>edu25</td>\n",
       "      <td>true</td>\n",
       "      <td>School Starts Too Early In The Morning, State ...</td>\n",
       "      <td>education</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10261</th>\n",
       "      <td>edu24</td>\n",
       "      <td>true</td>\n",
       "      <td>Hillary Clinton To Deliver Wellesley College's...</td>\n",
       "      <td>education</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10262</th>\n",
       "      <td>edu23</td>\n",
       "      <td>true</td>\n",
       "      <td>Toronto Cancels U.S. Class Trips Due To Trump ...</td>\n",
       "      <td>education</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10263 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           jsonid        label  \\\n",
       "0       2635.json        false   \n",
       "1      10540.json    half-true   \n",
       "2        324.json  mostly-true   \n",
       "3       1123.json        false   \n",
       "4       9028.json    half-true   \n",
       "5      12465.json         true   \n",
       "6       2342.json  barely-true   \n",
       "7        153.json    half-true   \n",
       "8       5602.json    half-true   \n",
       "9       9741.json  mostly-true   \n",
       "10      7115.json  mostly-true   \n",
       "11      4148.json    half-true   \n",
       "12      5947.json        false   \n",
       "13      8616.json  mostly-true   \n",
       "14      8705.json  barely-true   \n",
       "15     10683.json    half-true   \n",
       "16       620.json         true   \n",
       "17      3863.json  barely-true   \n",
       "18     12372.json    half-true   \n",
       "19     12385.json  mostly-true   \n",
       "20     10173.json        false   \n",
       "21      9867.json  mostly-true   \n",
       "22     12408.json  mostly-true   \n",
       "23      2673.json    half-true   \n",
       "24      7057.json  barely-true   \n",
       "25     10215.json        false   \n",
       "26     12517.json  mostly-true   \n",
       "27      3910.json    half-true   \n",
       "28     11092.json        false   \n",
       "29     12163.json  mostly-true   \n",
       "...           ...          ...   \n",
       "10233   4388.json   pants-fire   \n",
       "10234   1592.json    half-true   \n",
       "10235   5473.json  mostly-true   \n",
       "10236   3408.json  mostly-true   \n",
       "10237   3959.json    half-true   \n",
       "10238   2253.json        false   \n",
       "10239   1155.json   pants-fire   \n",
       "10240       edu27        false   \n",
       "10241       edu26        false   \n",
       "10242       edu25        false   \n",
       "10243       edu24        false   \n",
       "10244       edu23        false   \n",
       "10245       edu22        false   \n",
       "10246       edu21        false   \n",
       "10247       edu20        false   \n",
       "10248       edu19        false   \n",
       "10249       edu18        false   \n",
       "10250       edu17        false   \n",
       "10251       edu16        false   \n",
       "10252       edu15        false   \n",
       "10253       edu14        false   \n",
       "10254       edu31         true   \n",
       "10255       edu30         true   \n",
       "10256       edu29         true   \n",
       "10257       edu28         true   \n",
       "10258       edu27         true   \n",
       "10259       edu26         true   \n",
       "10260       edu25         true   \n",
       "10261       edu24         true   \n",
       "10262       edu23         true   \n",
       "\n",
       "                                           headline_text  \\\n",
       "0      Says the Annies List political group supports ...   \n",
       "1      When did the decline of coal start? It started...   \n",
       "2      Hillary Clinton agrees with John McCain \"by vo...   \n",
       "3      Health care reform legislation is likely to ma...   \n",
       "4      The economic turnaround started at the end of ...   \n",
       "5      The Chicago Bears have had more starting quart...   \n",
       "6      Jim Dunnam has not lived in the district he re...   \n",
       "7      I'm the only person on this stage who has work...   \n",
       "8      However, it took $19.5 million in Oregon Lotte...   \n",
       "9      Says GOP primary opponents Glenn Grothman and ...   \n",
       "10     For the first time in history, the share of th...   \n",
       "11     Since 2000, nearly 12 million Americans have s...   \n",
       "12     When Mitt Romney was governor of Massachusetts...   \n",
       "13     The economy bled $24 billion due to the govern...   \n",
       "14     Most of the (Affordable Care Act) has already ...   \n",
       "15     In this last election in November, ... 63 perc...   \n",
       "16     McCain opposed a requirement that the governme...   \n",
       "17     U.S. Rep. Ron Kind, D-Wis., and his fellow Dem...   \n",
       "18     Water rates in Manila, Philippines, were raise...   \n",
       "19     Almost 100,000 people left Puerto Rico last year.   \n",
       "20     Women and men both are making less when you ad...   \n",
       "21     The United States has the highest corporate ta...   \n",
       "22     We just had the best year for the auto industr...   \n",
       "23     Says Scott Walker favors cutting up to 350,000...   \n",
       "24     Says Mitt Romney wants to get rid of Planned P...   \n",
       "25                 I dont know who (Jonathan Gruber) is.   \n",
       "26     Hate crimes against American Muslims and mosqu...   \n",
       "27     Rick Perry has never lost an election and rema...   \n",
       "28     ISIS supporter tweeted at 10:34 a.m. Shooting ...   \n",
       "29     Youth unemployment in minority communities is ...   \n",
       "...                                                  ...   \n",
       "10233  Mayor Fung wants to punish our childrens educa...   \n",
       "10234  Under the ruling of the Supreme Court, any lob...   \n",
       "10235  There are a larger number of shark attacks in ...   \n",
       "10236  Democrats have now become the party of the [At...   \n",
       "10237  Says an alternative to Social Security that op...   \n",
       "10238  On lifting the U.S. Cuban embargo and allowing...   \n",
       "10239  The Department of Veterans Affairs has a manua...   \n",
       "10240  Harvard Law, Moving to Limit Applicant Pool, W...   \n",
       "10241  North Carolina teacher indoctrinates his stude...   \n",
       "10242  School Starts Too Late In The Morning, State L...   \n",
       "10243  Hillary Clinton: Keynote Speaker at Wellesley ...   \n",
       "10244  Torona Cancels U.S. Class Trips Due To Trump C...   \n",
       "10245  U.S. Schools Easy, Full of Sports, Says exchan...   \n",
       "10246  Donald Trump's Win To The Presidency Causes Fr...   \n",
       "10247  Google cancels future funding towards closing ...   \n",
       "10248  Nutrition has no effect on children's school p...   \n",
       "10249  Students wield guns dressed up like sex toys a...   \n",
       "10250  ILLEGAL IMMIGRANTS BEING RECRUITED TO FILL TEA...   \n",
       "10251  Obama Administration sets new rule for childre...   \n",
       "10252  Racial Bias Study Shows Bad Behavior Along Rac...   \n",
       "10253  Microsoft Aims to spread liberalism on the suc...   \n",
       "10254  Wi-Fi Microscopes Help Texas Students With Sci...   \n",
       "10255  Immigration Controversy Could Drive Up College...   \n",
       "10256             STEM Students Create Winning Invention   \n",
       "10257  Healthier Cereals Snare a Spot on New York Sch...   \n",
       "10258  Harvard Law, Moving to Diversify Applicant Poo...   \n",
       "10259  This Teacher Has Personalized Handshakes For A...   \n",
       "10260  School Starts Too Early In The Morning, State ...   \n",
       "10261  Hillary Clinton To Deliver Wellesley College's...   \n",
       "10262  Toronto Cancels U.S. Class Trips Due To Trump ...   \n",
       "\n",
       "                                                 subject  \\\n",
       "0                                               abortion   \n",
       "1                     energy,history,job-accomplishments   \n",
       "2                                         foreign-policy   \n",
       "3                                            health-care   \n",
       "4                                           economy,jobs   \n",
       "5                                              education   \n",
       "6                                   candidates-biography   \n",
       "7                                                 ethics   \n",
       "8                                                   jobs   \n",
       "9              energy,message-machine-2014,voting-record   \n",
       "10                                             elections   \n",
       "11               economy,jobs,new-hampshire-2012,poverty   \n",
       "12                                  history,state-budget   \n",
       "13                    economy,federal-budget,health-care   \n",
       "14                                           health-care   \n",
       "15                                             elections   \n",
       "16                                        federal-budget   \n",
       "17                                        federal-budget   \n",
       "18             financial-regulation,foreign-policy,water   \n",
       "19                         bankruptcy,economy,population   \n",
       "20                                        economy,income   \n",
       "21                                    corporations,taxes   \n",
       "22                                               economy   \n",
       "23                           health-care,message-machine   \n",
       "24                   abortion,federal-budget,health-care   \n",
       "25                                           health-care   \n",
       "26           crime,diversity,homeland-security,terrorism   \n",
       "27                                  candidates-biography   \n",
       "28                                  technology,terrorism   \n",
       "29                                diversity,economy,jobs   \n",
       "...                                                  ...   \n",
       "10233  children,city-budget,deficit,education,state-b...   \n",
       "10234                             corporations,elections   \n",
       "10235                                  animals,elections   \n",
       "10236                                          elections   \n",
       "10237                         retirement,social-security   \n",
       "10238                             florida,foreign-policy   \n",
       "10239                               health-care,veterans   \n",
       "10240                                          education   \n",
       "10241                                          education   \n",
       "10242                                          education   \n",
       "10243                                          education   \n",
       "10244                                          education   \n",
       "10245                                          education   \n",
       "10246                                          education   \n",
       "10247                                          education   \n",
       "10248                                          education   \n",
       "10249                                          education   \n",
       "10250                                          education   \n",
       "10251                                          education   \n",
       "10252                                          education   \n",
       "10253                                          education   \n",
       "10254                                          education   \n",
       "10255                                          education   \n",
       "10256                                          education   \n",
       "10257                                          education   \n",
       "10258                                          education   \n",
       "10259                                          education   \n",
       "10260                                          education   \n",
       "10261                                          education   \n",
       "10262                                          education   \n",
       "\n",
       "                                           speaker  \\\n",
       "0                                     dwayne-bohac   \n",
       "1                                   scott-surovell   \n",
       "2                                     barack-obama   \n",
       "3                                     blog-posting   \n",
       "4                                    charlie-crist   \n",
       "5                                        robin-vos   \n",
       "6                           republican-party-texas   \n",
       "7                                     barack-obama   \n",
       "8                                   oregon-lottery   \n",
       "9                                    duey-stroebel   \n",
       "10                                 robert-menendez   \n",
       "11                                        bernie-s   \n",
       "12                                     mitt-romney   \n",
       "13                                      doonesbury   \n",
       "14                                     george-will   \n",
       "15                                        bernie-s   \n",
       "16                                    barack-obama   \n",
       "17     national-republican-congressional-committee   \n",
       "18                                      gwen-moore   \n",
       "19                                        jack-lew   \n",
       "20                               dennis-richardson   \n",
       "21                                    eric-bolling   \n",
       "22                                 hillary-clinton   \n",
       "23                greater-wisconsin-political-fund   \n",
       "24                  planned-parenthood-action-fund   \n",
       "25                                    nancy-pelosi   \n",
       "26                                 hillary-clinton   \n",
       "27                                      ted-nugent   \n",
       "28                                   pamela-geller   \n",
       "29                                    peter-kinder   \n",
       "...                                            ...   \n",
       "10233                              richard-tomlins   \n",
       "10234                                david-axelrod   \n",
       "10235                                 aclu-florida   \n",
       "10236                                  alan-powell   \n",
       "10237                                  herman-cain   \n",
       "10238                                  jeff-greene   \n",
       "10239                               michael-steele   \n",
       "10240                                            0   \n",
       "10241                                            0   \n",
       "10242                                            0   \n",
       "10243                                            0   \n",
       "10244                                            0   \n",
       "10245                                            0   \n",
       "10246                                            0   \n",
       "10247                                            0   \n",
       "10248                                            0   \n",
       "10249                                            0   \n",
       "10250                                            0   \n",
       "10251                                            0   \n",
       "10252                                            0   \n",
       "10253                                            0   \n",
       "10254                                            0   \n",
       "10255                                            0   \n",
       "10256                                            0   \n",
       "10257                                            0   \n",
       "10258                                            0   \n",
       "10259                                            0   \n",
       "10260                                            0   \n",
       "10261                                            0   \n",
       "10262                                            0   \n",
       "\n",
       "                                         speakerjobtitle          stateinfo  \\\n",
       "0                                   State representative              Texas   \n",
       "1                                         State delegate           Virginia   \n",
       "2                                              President           Illinois   \n",
       "3                                                      0                  0   \n",
       "4                                                      0            Florida   \n",
       "5                             Wisconsin Assembly speaker          Wisconsin   \n",
       "6                                                      0              Texas   \n",
       "7                                              President           Illinois   \n",
       "8                                                      0                  0   \n",
       "9                                   State representative          Wisconsin   \n",
       "10                                          U.S. Senator         New Jersey   \n",
       "11                                          U.S. Senator            Vermont   \n",
       "12                                       Former governor      Massachusetts   \n",
       "13                                                     0                  0   \n",
       "14                                             Columnist           Maryland   \n",
       "15                                          U.S. Senator            Vermont   \n",
       "16                                             President           Illinois   \n",
       "17                                                     0                  0   \n",
       "18                     U.S. House member -- 4th District          Wisconsin   \n",
       "19                                   Treasury secretary   Washington, D.C.    \n",
       "20                                  state representative             Oregon   \n",
       "21              Co-host on Fox News Channel's \"The Five\"                  0   \n",
       "22                                Presidential candidate           New York   \n",
       "23                                                     0          Wisconsin   \n",
       "24                                        Advocacy group   Washington, D.C.   \n",
       "25                                 House Minority Leader         California   \n",
       "26                                Presidential candidate           New York   \n",
       "27                                              musician              Texas   \n",
       "28     President of American Freedom Defense Intitiative           New York   \n",
       "29                                   Lieutenant governor           Missouri   \n",
       "...                                                  ...                ...   \n",
       "10233                                                  0       Rhode Island   \n",
       "10234                                                  0                  0   \n",
       "10235                                                  0            Florida   \n",
       "10236                                                  0            Georgia   \n",
       "10237                                                  0            Georgia   \n",
       "10238                                                  0            Florida   \n",
       "10239      chairman of the Republican National Committee           Maryland   \n",
       "10240                                                  0                  0   \n",
       "10241                                                  0                  0   \n",
       "10242                                                  0                  0   \n",
       "10243                                                  0                  0   \n",
       "10244                                                  0                  0   \n",
       "10245                                                  0                  0   \n",
       "10246                                                  0                  0   \n",
       "10247                                                  0                  0   \n",
       "10248                                                  0                  0   \n",
       "10249                                                  0                  0   \n",
       "10250                                                  0                  0   \n",
       "10251                                                  0                  0   \n",
       "10252                                                  0                  0   \n",
       "10253                                                  0                  0   \n",
       "10254                                                  0                  0   \n",
       "10255                                                  0                  0   \n",
       "10256                                                  0                  0   \n",
       "10257                                                  0                  0   \n",
       "10258                                                  0                  0   \n",
       "10259                                                  0                  0   \n",
       "10260                                                  0                  0   \n",
       "10261                                                  0                  0   \n",
       "10262                                                  0                  0   \n",
       "\n",
       "      partyaffiliation  barelytruecounts  falsecounts  halftruecounts  \\\n",
       "0           republican               0.0          1.0             0.0   \n",
       "1             democrat               0.0          0.0             1.0   \n",
       "2             democrat              70.0         71.0           160.0   \n",
       "3                 none               7.0         19.0             3.0   \n",
       "4             democrat              15.0          9.0            20.0   \n",
       "5           republican               0.0          3.0             2.0   \n",
       "6           republican               3.0          1.0             1.0   \n",
       "7             democrat              70.0         71.0           160.0   \n",
       "8         organization               0.0          0.0             1.0   \n",
       "9           republican               0.0          0.0             0.0   \n",
       "10            democrat               1.0          3.0             1.0   \n",
       "11         independent              18.0         12.0            22.0   \n",
       "12          republican              34.0         32.0            58.0   \n",
       "13                none               0.0          0.0             2.0   \n",
       "14           columnist               7.0          6.0             3.0   \n",
       "15         independent              18.0         12.0            22.0   \n",
       "16            democrat              70.0         71.0           160.0   \n",
       "17          republican              18.0          9.0             8.0   \n",
       "18            democrat               3.0          4.0             4.0   \n",
       "19            democrat               0.0          1.0             0.0   \n",
       "20          republican               0.0          4.0             1.0   \n",
       "21                none               2.0          1.0             1.0   \n",
       "22            democrat              40.0         29.0            69.0   \n",
       "23                none               3.0          3.0             3.0   \n",
       "24                none               1.0          0.0             0.0   \n",
       "25            democrat               3.0          7.0            11.0   \n",
       "26            democrat              40.0         29.0            69.0   \n",
       "27          republican               0.0          0.0             2.0   \n",
       "28            activist               0.0          1.0             1.0   \n",
       "29          republican               0.0          0.0             0.0   \n",
       "...                ...               ...          ...             ...   \n",
       "10233         democrat               0.0          0.0             0.0   \n",
       "10234         democrat               2.0          1.0             6.0   \n",
       "10235             none               0.0          1.0             1.0   \n",
       "10236       republican               0.0          0.0             0.0   \n",
       "10237       republican               4.0         11.0             5.0   \n",
       "10238         democrat               3.0          1.0             3.0   \n",
       "10239       republican               0.0          1.0             1.0   \n",
       "10240                0               0.0          0.0             0.0   \n",
       "10241                0               0.0          0.0             0.0   \n",
       "10242                0               0.0          0.0             0.0   \n",
       "10243                0               0.0          0.0             0.0   \n",
       "10244                0               0.0          0.0             0.0   \n",
       "10245                0               0.0          0.0             0.0   \n",
       "10246                0               0.0          0.0             0.0   \n",
       "10247                0               0.0          0.0             0.0   \n",
       "10248                0               0.0          0.0             0.0   \n",
       "10249                0               0.0          0.0             0.0   \n",
       "10250                0               0.0          0.0             0.0   \n",
       "10251                0               0.0          0.0             0.0   \n",
       "10252                0               0.0          0.0             0.0   \n",
       "10253                0               0.0          0.0             0.0   \n",
       "10254                0               0.0          0.0             0.0   \n",
       "10255                0               0.0          0.0             0.0   \n",
       "10256                0               0.0          0.0             0.0   \n",
       "10257                0               0.0          0.0             0.0   \n",
       "10258                0               0.0          0.0             0.0   \n",
       "10259                0               0.0          0.0             0.0   \n",
       "10260                0               0.0          0.0             0.0   \n",
       "10261                0               0.0          0.0             0.0   \n",
       "10262                0               0.0          0.0             0.0   \n",
       "\n",
       "       mostlytrueocunts  pantsonfirecounts  \\\n",
       "0                   0.0                0.0   \n",
       "1                   1.0                0.0   \n",
       "2                 163.0                9.0   \n",
       "3                   5.0               44.0   \n",
       "4                  19.0                2.0   \n",
       "5                   5.0                1.0   \n",
       "6                   3.0                1.0   \n",
       "7                 163.0                9.0   \n",
       "8                   0.0                1.0   \n",
       "9                   1.0                0.0   \n",
       "10                  3.0                0.0   \n",
       "11                 41.0                0.0   \n",
       "12                 33.0               19.0   \n",
       "13                  4.0                0.0   \n",
       "14                  5.0                1.0   \n",
       "15                 41.0                0.0   \n",
       "16                163.0                9.0   \n",
       "17                  5.0                8.0   \n",
       "18                  3.0                1.0   \n",
       "19                  1.0                0.0   \n",
       "20                  2.0                0.0   \n",
       "21                  1.0                0.0   \n",
       "22                 76.0                7.0   \n",
       "23                  1.0                1.0   \n",
       "24                  0.0                0.0   \n",
       "25                  2.0                3.0   \n",
       "26                 76.0                7.0   \n",
       "27                  0.0                2.0   \n",
       "28                  0.0                0.0   \n",
       "29                  1.0                1.0   \n",
       "...                 ...                ...   \n",
       "10233               0.0                1.0   \n",
       "10234               7.0                0.0   \n",
       "10235               1.0                0.0   \n",
       "10236               1.0                0.0   \n",
       "10237               3.0                3.0   \n",
       "10238               0.0                0.0   \n",
       "10239               0.0                2.0   \n",
       "10240               0.0                0.0   \n",
       "10241               0.0                0.0   \n",
       "10242               0.0                0.0   \n",
       "10243               0.0                0.0   \n",
       "10244               0.0                0.0   \n",
       "10245               0.0                0.0   \n",
       "10246               0.0                0.0   \n",
       "10247               0.0                0.0   \n",
       "10248               0.0                0.0   \n",
       "10249               0.0                0.0   \n",
       "10250               0.0                0.0   \n",
       "10251               0.0                0.0   \n",
       "10252               0.0                0.0   \n",
       "10253               0.0                0.0   \n",
       "10254               0.0                0.0   \n",
       "10255               0.0                0.0   \n",
       "10256               0.0                0.0   \n",
       "10257               0.0                0.0   \n",
       "10258               0.0                0.0   \n",
       "10259               0.0                0.0   \n",
       "10260               0.0                0.0   \n",
       "10261               0.0                0.0   \n",
       "10262               0.0                0.0   \n",
       "\n",
       "                                                 context  \n",
       "0                                               a mailer  \n",
       "1                                        a floor speech.  \n",
       "2                                                 Denver  \n",
       "3                                         a news release  \n",
       "4                                    an interview on CNN  \n",
       "5                              a an online opinion-piece  \n",
       "6                                       a press release.  \n",
       "7               a Democratic debate in Philadelphia, Pa.  \n",
       "8                                             a website   \n",
       "9                                        an online video  \n",
       "10                                              a speech  \n",
       "11                                               a tweet  \n",
       "12                            an interview with CBN News  \n",
       "13               a Doonesbury strip in the Sunday comics  \n",
       "14                         comments on \"Fox News Sunday\"  \n",
       "15                          a town hall in Austin, Texas  \n",
       "16                                            a radio ad  \n",
       "17                                        a news release  \n",
       "18                               a congressional hearing  \n",
       "19                      an interview with Bloomberg News  \n",
       "20                                     a campaign debate  \n",
       "21                  a discussion on Fox News' \"The Five\"  \n",
       "22                           remarks at a Kentucky rally  \n",
       "23                                      a campaign TV ad  \n",
       "24                                            a radio ad  \n",
       "25                                     a news conference  \n",
       "26          a speech after a terrorist attack in Orlando  \n",
       "27                                       an oped column.  \n",
       "28                                              a tweet   \n",
       "29                                a gubernatorial debate  \n",
       "...                                                  ...  \n",
       "10233  a recorded telephone message to Cranston resid...  \n",
       "10234                    an interview on ABC's This Week  \n",
       "10235                  interview on \"The Colbert Report\"  \n",
       "10236                                       an interview  \n",
       "10237                   a Republican presidential debate  \n",
       "10238  a televised debate on Miami's WPLG-10 against ...  \n",
       "10239                               a Fox News interview  \n",
       "10240                                                  0  \n",
       "10241                                                  0  \n",
       "10242                                                  0  \n",
       "10243                                                  0  \n",
       "10244                                                  0  \n",
       "10245                                                  0  \n",
       "10246                                                  0  \n",
       "10247                                                  0  \n",
       "10248                                                  0  \n",
       "10249                                                  0  \n",
       "10250                                                  0  \n",
       "10251                                                  0  \n",
       "10252                                                  0  \n",
       "10253                                                  0  \n",
       "10254                                                  0  \n",
       "10255                                                  0  \n",
       "10256                                                  0  \n",
       "10257                                                  0  \n",
       "10258                                                  0  \n",
       "10259                                                  0  \n",
       "10260                                                  0  \n",
       "10261                                                  0  \n",
       "10262                                                  0  \n",
       "\n",
       "[10263 rows x 14 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Text Preprocessing \n",
    "\n",
    "*Steps included in the preprocessing:*\n",
    "- Remove Special Characters and Punctuations\n",
    "- Lower case the news\n",
    "- Tokenization\n",
    "- Remove Stop Words\n",
    "- Lemmatization\n",
    "- Stemming \n",
    "- Spell Check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def cleaning(raw_news):\n",
    "    import nltk\n",
    "    \n",
    "    # 1. Remove non-letters/Special Characters and Punctuations\n",
    "    news = re.sub(\"[^a-zA-Z]\", \" \", raw_news)\n",
    "    \n",
    "    # 2. Convert to lower case.\n",
    "    news =  news.lower()\n",
    "    \n",
    "    # 3. Tokenize.\n",
    "    news_words = nltk.word_tokenize( news)\n",
    "    \n",
    "    # 4. Convert the stopwords list to \"set\" data type.\n",
    "    stops = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "    \n",
    "    # 5. Remove stop words. \n",
    "    words = [w for w in  news_words  if not w in stops]\n",
    "    \n",
    "    # 6. Lemmentize \n",
    "    wordnet_lem = [ WordNetLemmatizer().lemmatize(w) for w in words ]\n",
    "    \n",
    "    # 7. Stemming\n",
    "    stems = [nltk.stem.SnowballStemmer('english').stem(w) for w in wordnet_lem ]\n",
    "    \n",
    "    # 8. Join the stemmed words back into one string separated by space, and return the result.\n",
    "    return \" \".join(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-3e03fa885ae1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Add the processed data to the original data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Perhaps using apply function would be more elegant and concise than using for loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrain_news\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clean'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_news\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"headline_text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/pandas/core/series.pyc\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   3192\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3193\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3194\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3196\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/src/inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-0c2d900ace6c>\u001b[0m in \u001b[0;36mcleaning\u001b[0;34m(raw_news)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# 5. Remove stop words.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m  \u001b[0mnews_words\u001b[0m  \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstops\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# 6. Lemmentize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "# clean training and test data \n",
    "# create new column \"tokenized\"\n",
    "t1 = time.time()\n",
    "\n",
    "# Add the processed data to the original data. \n",
    "# Perhaps using apply function would be more elegant and concise than using for loop\n",
    "train_news['clean'] = train_news[\"headline_text\"].apply(cleaning) \n",
    "\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"\\nTime to clean, tokenize and stem train data: \\n\", len(train_news), \"news:\", (t2-t1)/60, \"min\")\n",
    "\n",
    "t1 = time.time()\n",
    "test_news['clean'] = test_news[\"headline_text\"].apply(cleaning)\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"\\n\\nTime to clean, tokenize and stem test data: \\n\", len(test_news), \"news:\", (t2-t1)/60, \"min\")\n",
    "\n",
    "t1 = time.time()\n",
    "valid_news['clean'] = valid_news[\"headline_text\"].apply(cleaning)\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"\\n\\nTime to clean, tokenize and stem valid data: \\n\", len(valid_news), \"news:\", (t2-t1)/60, \"min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Google News corpus word2vec](http://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/)\n",
    "\n",
    "### Spell Check \n",
    "\n",
    "-  You can download the pre-trained model [**here**](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit)\n",
    "\n",
    "- Or clone it from GitHub [**GoogleNews-vectors-negative300**](https://github.com/mmihaltz/word2vec-GoogleNews-vectors)\n",
    "\n",
    "> Itâ€™s 1.5GB! It includes word vectors for a vocabulary of 3 million words and phrases that they trained on roughly 100 billion words from a Google News dataset. The vector length is 300 features.\n",
    "\n",
    "**3 million words * 300 features * 4bytes/feature = ~3.35GB**\n",
    "\n",
    "> This file consist of the word2vec -  pre-trained Google News corpus (3 billion running words) to word vector model (3 million 300-dimension English word vectors).\n",
    "\n",
    "> Look at the [**vocabulory list**](https://github.com/chrisjmccormick/inspect_word2vec/tree/master/vocabulary) used to train this model. Each text file contains 100,000 entries from the model. \n",
    "\n",
    "\n",
    ">  There are few things that this dataset contains and not. It has stop words like  â€œtheâ€, â€œalsoâ€, â€œshouldâ€ and does not have stop words like â€œaâ€, â€œandâ€, â€œofâ€. As I have removed the stop words the complexity is reduced as there is no need to check the spelling for stop words. \n",
    "\n",
    "> It does have numbers but in the form of entried wiht #. e.g., you wonâ€™t find â€œ100â€. But it does include entries like â€œ###MHz_DDR2_SDRAMâ€. \n",
    "\n",
    "The model used [**WinPython-64bit-2.7.10.3**](https://winpython.github.io/) for efficient python distribution on Windows system. Helps to run the scripts in batches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('input_data/GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "words = model.index2word\n",
    "\n",
    "w_rank = {}\n",
    "for i,word in enumerate(words):\n",
    "    w_rank[word] = i\n",
    "\n",
    "WORDS = w_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "def P(word, N=sum(WORDS.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    return - WORDS.get(word, 0)\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_checker(text):\n",
    "    all_words = re.findall(r'\\w+', text.lower()) # split sentence to words\n",
    "    spell_checked_text  = []\n",
    "    for i in range(len(all_words)):\n",
    "        spell_checked_text.append(correction(all_words[i]))\n",
    "    return ' '.join(spell_checked_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before: \\n\", train_news['clean'][0] )\n",
    "t1 = time.time()\n",
    "train_news['clean'] = train_news['clean'].apply(spell_checker)\n",
    "t2 = time.time()\n",
    "print(\"\\nTime to spell check the train data: \\n\", len(train_news), \"news:\", (t2-t1)/60, \"min\")\n",
    "\n",
    "print(\"\\nAfter: \\n\",train_news['clean'][0] )\n",
    "train_news.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "test_news['clean'] = test_news['clean'].apply(spell_checker)\n",
    "test_news.head(5)\n",
    "t2 = time.time()\n",
    "print(\"\\nTime to spell check the test data: \\n\", len(test_news), \"news:\", (t2-t1)/60, \"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "valid_news['clean'] = valid_news['clean'].apply(spell_checker)\n",
    "valid_news.head(5)\n",
    "t2 = time.time()\n",
    "print(\"\\nTime to spell check the valid data: \\n\", len(valid_news), \"news:\", (t2-t1)/60, \"min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saved the trained dataset into a seperate CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the data for education related subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_news.to_csv(\"input_data/train_processed.csv\", sep=',')\n",
    "test_news.to_csv(\"input_data/test_processed.csv\", sep=',')\n",
    "valid_news.to_csv(\"input_data/valid_processed.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_news.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edu_only_train = train_news[train_news['subject'] == 'education']\n",
    "edu_only_test = test_news[test_news['subject'] == 'education']\n",
    "edu_only_valid = valid_news[valid_news['subject'] == 'education']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization using word cloud on specifc feature of Education "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cloud(data,backgroundcolor = 'white', width = 800, height = 600):\n",
    "    wordcloud = WordCloud(stopwords = STOPWORDS, background_color = backgroundcolor,\n",
    "                         width = width, height = height).generate(data)\n",
    "    plt.figure(figsize = (15, 10))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "cloud(' '.join(edu_only_train['clean']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud(' '.join(edu_only_test['clean']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud(' '.join(edu_only_valid['clean']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature 1: Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running sentiment analysis using the education enriched data, anf focusing on data that specifically partains to education as the subject. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edu_only_train['clean'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import nltk.sentiment\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "senti = nltk.sentiment.vader.SentimentIntensityAnalyzer()\n",
    "\n",
    "def print_sentiment_scores(sentence):\n",
    "    snt = senti.polarity_scores(sentence)\n",
    "    print(\"{:-<40} \\n{}\".format(sentence, str(snt)))\n",
    "    \n",
    "print_sentiment_scores(edu_only_train['clean'][97])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vader_polarity(snt):\n",
    "    if not snt:\n",
    "        return None\n",
    "    elif snt['neg'] > snt['pos'] and snt['neg'] > snt['neu']:\n",
    "        return -1\n",
    "    elif snt['pos'] > snt['neg'] and snt['pos'] > snt['neu']:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to determine if a text is negative(-1) or postive (1) or neutral (0)\n",
    "def get_polarity_type(sentence):\n",
    "    sentimentVector = []\n",
    "    snt = senti.polarity_scores(sentence)\n",
    "    sentimentVector.append(get_vader_polarity(snt))\n",
    "    sentimentVector.append(snt['neg'])\n",
    "    sentimentVector.append(snt['neu'])\n",
    "    sentimentVector.append(snt['pos'])\n",
    "    sentimentVector.append(snt['compound'])\n",
    "    \n",
    "    print(sentimentVector)\n",
    "    return sentimentVector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- senti.polarity_scores is a dictionary\n",
    "- pos and neg indicates - positive and negative emotions in sentence\n",
    "- we should be interested in compound score which calculates the final effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = []\n",
    "vader_pol = []\n",
    "cmp_score = []\n",
    "for row in edu_only_train['clean']:\n",
    "    get_pols = get_polarity_type(row)\n",
    "    sentiment.append(get_pols[1:])\n",
    "    vader_pol.append(get_pols[0])\n",
    "    cmp_score.append(get_pols[1:][-1]) #last element \n",
    "    \n",
    "edu_only_train['sentiment_vector'] = sentiment\n",
    "edu_only_train['vader_polarity'] = vader_pol\n",
    "edu_only_train['sentiment_score'] = cmp_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edu_only_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edu_only_train['sentiment_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = []\n",
    "vader_pol = []\n",
    "cmp_score = []\n",
    "\n",
    "for row in edu_only_test['clean']:\n",
    "    get_pols = get_polarity_type(row)\n",
    "    sentiment.append(get_pols[1:])\n",
    "    vader_pol.append(get_pols[0])\n",
    "    cmp_score.append(get_pols[1:][-1]) #last element \n",
    "    \n",
    "edu_only_test['sentiment_vector'] = sentiment\n",
    "edu_only_test['vader_polarity'] = vader_pol\n",
    "edu_only_test['sentiment_score'] = cmp_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = []\n",
    "vader_pol = []\n",
    "cmp_score = []\n",
    "\n",
    "for row in edu_only_valid['clean']:\n",
    "    get_pols = get_polarity_type(row)\n",
    "    sentiment.append(get_pols[1:])\n",
    "    vader_pol.append(get_pols[0])\n",
    "    cmp_score.append(get_pols[1:][-1]) #last element \n",
    "    \n",
    "    \n",
    "edu_only_valid['sentiment_vector'] = sentiment\n",
    "edu_only_valid['vader_polarity'] = vader_pol\n",
    "edu_only_valid['sentiment_score'] = cmp_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edu_only_train.to_csv(\"input_data/train_sentiment.csv\", sep=',')\n",
    "edu_only_test.to_csv(\"input_data/test_sentiment.csv\", sep=',')\n",
    "edu_only_valid.to_csv(\"input_data/valid_sentiment.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature 2:  LDA Topic Modelling \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edu_only_train['index'] = edu_only_train.index\n",
    "data = edu_only_train\n",
    "train_lda = data[['clean', 'index']]\n",
    "train_lda.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edu_only_test['index'] = edu_only_test.index\n",
    "data = edu_only_test\n",
    "test_lda = data[['clean', 'index']]\n",
    "test_lda.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edu_only_valid['index'] = edu_only_valid.index\n",
    "data = edu_only_valid\n",
    "test_lda = data[['clean', 'index']]\n",
    "test_lda.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the clean news into list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = train_lda['clean'].map(lambda doc: doc.split(\" \"))\n",
    "processed_docs[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "> It is an example of a probabilistic topic model. Topic models are a great way to automatically explore and structure a large set of documents: they group or cluster documents based on the words that occur in them. As documents on similar topics tend to use a similar sub-vocabulary, the resulting clusters of documents can be interpreted as discussing different 'topics'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_tokens(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if len(token) > 3:\n",
    "            result.append(token)\n",
    "    return result\n",
    "tokenized_docs_local = edu_only_train['clean'].map(get_word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a function to build the dictionary and tokenized docs for given feature\n",
    "\n",
    "Below function does the following\n",
    "* #### Dictionary\n",
    "Returns Dictionary given, dataframe and column name\n",
    "* #### Tokenizeddocs\n",
    "Returns Tokenizeddocs, of the all the words in a text in that column can be used for bow_corpus\n",
    "* #### Dictionary is filtered using Gensim filter_extremes\n",
    "    Filter out tokens that appear in less than 15 documents (absolute number) or more than 0.5 documents (fraction of total corpus size, not absolute number). after the above two steps, keep only the first 100000 most frequent tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dictionary_print_words(dataframe,colname):\n",
    "    dictionary_gensim = gensim.corpora.Dictionary(processed_docs)\n",
    "    count = 0\n",
    "    print('######## DICTIONARY Words and occurences ########')\n",
    "    for k, v in dictionary_gensim.iteritems():\n",
    "        print(k, v)\n",
    "        count += 1\n",
    "        if count > 10:\n",
    "            break\n",
    "    dictionary_gensim.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "    return dictionary_gensim, tokenized_docs_local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gensim filter_extremes\n",
    "\n",
    "> Filter out tokens that appear less than 15 documents (absolute number) or more than 0.5 documents (fraction of total corpus size, not absolute number). after the above two steps, keep only the first 100000 most frequent tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a function to build bow_corpus from dictionary and tokenized_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bow_corpus_print_sample(dataframe,colname):\n",
    "    dictionary_gensim, tokenized_docs_local = get_dictionary_print_words(dataframe, colname)\n",
    "    bow_corpus_local = [dictionary_gensim.doc2bow(doc) for doc in tokenized_docs_local]\n",
    "    bow_doc_local_0 = bow_corpus_local[0]\n",
    "    print('\\n ######## BOW VECTOR FIRST ITEM ########')\n",
    "    print(bow_doc_local_0)\n",
    "    print('\\n ######## PREVIEW BOW ########')\n",
    "    for i in range(len(bow_doc_local_0)):\n",
    "        print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_local_0[i][0], \n",
    "                                               dictionary_gensim[bow_doc_local_0[i][0]], bow_doc_local_0[i][1]))\n",
    "    return bow_corpus_local, dictionary_gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gensim doc2bow**\n",
    "\n",
    "For each document we create a dictionary reporting how many words and how many times those words appear. Save this to â€˜bow_corpusâ€™, then check our selected document earlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a function to build tfidf_corpus from bow_corpus\n",
    "\n",
    "Create tf-idf model object using models.TfidfModel on â€˜bow_corpusâ€™ and save it to â€˜tfidfâ€™, then apply transformation to the entire corpus and call it â€˜corpus_tfidfâ€™. Finally we preview TF-IDF scores for our first document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf_corpus_print_sample(bow_corpus_local):\n",
    "    from gensim import corpora, models\n",
    "    tfidf = models.TfidfModel(bow_corpus_local)\n",
    "    tfidf_corpus_local = tfidf[bow_corpus_local]\n",
    "    print('\\n ######## TFIDF VECTOR FIRST ITEM ########')\n",
    "    \n",
    "    from pprint import pprint\n",
    "    for doc in tfidf_corpus_local:\n",
    "        pprint(doc)\n",
    "        break\n",
    "    return tfidf_corpus_local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a function to run ldamodel and print top 10 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lda_model_print_top_topics(bow_corpusforlda,numtopics,dictionaryforlda):\n",
    "    lda_model = gensim.models.LdaMulticore(bow_corpusforlda, num_topics=numtopics, id2word=dictionaryforlda, passes=2, workers=2)\n",
    "    lda_all_topics=lda_model.show_topics(num_topics=numtopics, num_words=10,formatted=False)\n",
    "    lda_topics_words = [(tp[0], [wd[0] for wd in tp[1]]) for tp in lda_all_topics]\n",
    "\n",
    "    #Below Code Prints Topics and Words\n",
    "    for topic,words in lda_topics_words:\n",
    "        print(str(topic)+ \"::\"+ str(words))\n",
    "    return lda_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a function to run ldamodel and print top 10 topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lda_model_topics_topwords_print_top_topics(bow_corpusforlda,numtopics,dictionaryforlda):\n",
    "    lda_model = gensim.models.LdaMulticore(bow_corpusforlda, num_topics=numtopics, id2word=dictionaryforlda, passes=2, workers=2, random_state=1)\n",
    "    lda_all_topics=lda_model.show_topics(num_topics=numtopics, num_words=10,formatted=False)\n",
    "    lda_topics_words = [(tp[0], [wd[0] for wd in tp[1]]) for tp in lda_all_topics]\n",
    "\n",
    "    #Below Code Prints Topics and Words\n",
    "    for topic,words in lda_topics_words:\n",
    "        print(str(topic)+ \"::\"+ str(words))\n",
    "    return lda_model,lda_topics_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a function to enrich data with lda topics, lda topics score, top words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_topic_number_score_label_topwords(text,dictionary_local,lda_model_local,lda_topics_top_words_local):\n",
    "    bow_vector_local = dictionary_local.doc2bow(get_word_tokens(text))\n",
    "    topic_number_local, topic_score_local = sorted(\n",
    "        lda_model_local[bow_vector_local], key=lambda tup: -1*tup[1])[0]\n",
    "    #print (topic_number_local, topic_score_local)\n",
    "    return pd.Series([topic_number_local, topic_score_local,\" \".join(lda_topics_top_words_local[int(topic_number_local)][1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a function that can enrich topic data to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_lda_results_to_dataset(dataframe,topiccolnames,coltoapplylda,colnamedictionary,colnameldamodel, colnameldatopwords):\n",
    "    dataframe[topiccolnames] = dataframe.apply(\n",
    "    lambda row: identify_topic_number_score_label_topwords(\n",
    "        row[coltoapplylda],colnamedictionary,colnameldamodel,\n",
    "        colnameldatopwords), axis=1)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words\n",
    "\n",
    "#### Create a dictionary and tokens\n",
    "\n",
    "> Create a dictionary from â€˜processed_docsâ€™ containing the number of times a word appears in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a function to convert text to word tokens from cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus_headline, dictionary_headline = get_bow_corpus_print_sample(edu_only_train,\n",
    "                                                                      'clean')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running LDA using Bag of Words\n",
    "Train our lda model using gensim.models.LdaMulticore and save it to â€˜lda_modelâ€™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_headline, lda_headline_topic_words = get_lda_model_topics_topwords_print_top_topics(\n",
    "    bow_corpus_headline, 10 ,dictionary_headline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_headline.top_topics(bow_corpus_headline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate TF-IDF bow_corpus\n",
    "Create tf-idf model object using models.TfidfModel on â€˜bow_corpusâ€™ and save it to â€˜tfidfâ€™, then apply transformation to the entire corpus and call it â€˜corpus_tfidfâ€™. Finally we preview TF-IDF scores for our first document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_corpus_headline = get_tfidf_corpus_print_sample(bow_corpus_headline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running LDA model using Bag of Words\n",
    "Train our lda model using gensim.models.LdaMulticore and save it to â€˜lda_modelâ€™\n",
    "\n",
    "**GOAL**: To get top ten topics with top words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_tfidf_model_headline  = get_lda_model_print_top_topics(tfidf_corpus_headline,10,dictionary_headline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_tfidf_model_headline.get_topics()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lda_tfidf_model_headline.print_topics()[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semisupervised Labeling\n",
    "Based on train,test and valid data explored the topic scores for sample data and identified below topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semisupervised_topic_labels = ['topic0','topic1','topic2','topic3','topic4','topic5','topic6','topic7','topic8','topic9']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Function to add topicnumber, topicscore, topiclabel, topwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlinetopiccolnames = ['topic_number','lda_score','topic_top_words']\n",
    "edu_only_train = update_lda_results_to_dataset(\n",
    "    edu_only_train, headlinetopiccolnames,'clean', dictionary_headline, lda_model_headline, lda_headline_topic_words)\n",
    "edu_only_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edu_only_test = update_lda_results_to_dataset(\n",
    "    edu_only_test,headlinetopiccolnames,'clean',\n",
    "  dictionary_headline,lda_model_headline,lda_headline_topic_words)\n",
    "edu_only_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edu_only_valid = update_lda_results_to_dataset(\n",
    "    edu_only_valid,headlinetopiccolnames,'clean',\n",
    "  dictionary_headline,lda_model_headline,lda_headline_topic_words)\n",
    "edu_only_valid.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the LDA Distribution of news against Top 10 Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GOAL 1:** *Each of the N documents will be represented in the LDA model by a vector of length M*\n",
    "**GOAL 2:** *Each of the M topics is represented by a vector of length V*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sb\n",
    "def create_distribution(dataFile):\n",
    "    g = sb.countplot(x='topic_number', data=dataFile, palette='hls')\n",
    "    g.set_xticklabels(g.get_xticklabels(),rotation=90)\n",
    "\n",
    "    return g\n",
    "\n",
    "create_distribution(edu_only_train) # TRAIN Document Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_distribution(edu_only_test) # TRAIN Document Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_distribution(edu_only_valid) # TRAIN Document Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Saved the latest dataset into a seperate CSV file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edu_only_train.to_csv(\"input_data/train_lda.csv\", sep=',')\n",
    "edu_only_test.to_csv(\"input_data/test_lda.csv\", sep=',')\n",
    "edu_only_valid.to_csv(\"input_data/valid_lda.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature 3:  Sensational Feature Analysis\n",
    "\n",
    "#### [Sensational Words Corpus](https://www.thepersuasionrevolution.com/380-high-emotion-persuasive-words/)\n",
    "\n",
    ">  Words arenâ€™t just strings of alphabets sewn together with ink. Words are cues. Words are triggers. Words when used correctly can transform an â€œeh whateverâ€ into â€œwow thatâ€™s it!â€. Words can make you go from literally ROFL to fuming with fury to an uncontrollable-urge-to-take-action-NOW-or-the-earth-may-stop-swinging -on-its-axis.\n",
    "\n",
    "> Highly emotional words are capable capable of transforming an absolute no into almost yes and a â€œperhapsâ€ into â€œfor sureâ€!\n",
    "\n",
    "Words that are used:\n",
    "- When you are trying to sell people a solution\n",
    "- When you are trying to get them to take an action (like, share, subscribe, buy)\n",
    "- When you are trying to get people to click and read your article\n",
    "- When you are trying to get someone to agree with you\n",
    "\n",
    "**There are 1400+ words that are both positive and negative emotions that will help to predict the sensational score for an article**\n",
    "\n",
    "> I have used these words to perform cosin similarity and predict the sensational similarity score for each news in the give dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for i in edu_only_train['clean']:\n",
    "    corpus.append(i)\n",
    "# corpus\n",
    "\n",
    "sensational_corpus=[]\n",
    "sensational_words = pd.read_csv('input_data/sensational_words/sensational_words_dict.csv', sep=\"\\t+\", header=None, usecols=[0] )\n",
    "print(len(sensational_words))\n",
    "sensational_dictionary = ' '.join(sensational_words[0].astype(str))\n",
    "sensational_corpus.append(sensational_dictionary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF and Cosine Similarity\n",
    "\n",
    "#### TF-IDF\n",
    "\n",
    "> TF-IDF (Term Frequency - Inverse Document Frequency) can be represented tf(d,t) X idf(t). TF-IDF uses the method diminishing the weight (importance) of words appeared in many documents in common, considered them incapable of discerning the documents, rather than simply counting the frequency of words as CountVectorizer does. The outcome matrix consists of each document (row) and each word (column) and the importance (weight) computed by tf * idf (values of the matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidfVec = TfidfVectorizer(max_features=1000)\n",
    "corpus = []\n",
    "for i in edu_only_train['clean']:\n",
    "    corpus.append(i)\n",
    "\n",
    "tfidf_corpus = tfidfVec.fit_transform(corpus)\n",
    "tf_idf_senti = tfidfVec.fit_transform(sensational_corpus)\n",
    "words = tfidfVec.get_feature_names()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_corpus.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_senti.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfVec.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfidf = tfidfVec.fit_transform(edu_only_train['clean'])\n",
    "print('Training dim:', train_tfidf.shape)\n",
    "print(train_tfidf.A[:10])\n",
    "\n",
    "\n",
    "test_tfidf = tfidfVec.fit_transform(edu_only_test['clean'])\n",
    "print('Test dim:', test_tfidf.shape)\n",
    "print(test_tfidf.A[:10])\n",
    "\n",
    "\n",
    "valid_tfidf = tfidfVec.fit_transform(edu_only_valid['clean'])\n",
    "print('Valid dim:', valid_tfidf.shape)\n",
    "print(valid_tfidf.A[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Cosine Similarity Score\n",
    "\n",
    "> The cosine similarity between two vectors (or two documents on the Vector Space) is a measure that calculates the cosine of the angle between them. This metric is a measurement of orientation and not magnitude, it can be seen as a comparison between documents on a normalized space because weâ€™re not taking into the consideration only the magnitude of each word count (tf-idf) of each document, but the angle between the documents.\n",
    "\n",
    "> I have compared the sentiment vector of each doucment and estimated a similarity score which is saved as a column in the training and test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import spatial\n",
    "similarity_score = []\n",
    "for i in range(len(train_tfidf.toarray())):\n",
    "    similarity_score.append(1 - spatial.distance.cosine(tf_idf_senti[0].toarray(), tfidf_corpus[i].toarray()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature 4: Political Affiliation Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import  LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import *\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import  LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#from sklearn.cross_validation import KFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import learning_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "class PartyAffiliation():\n",
    "    \n",
    "    # API to check whether the subject(Headline) is present in the \n",
    "    # - democrats most used words if the party affiliation is democrat\n",
    "    # - republicans most used words if the part affiliation is republican\n",
    "    def partyAffiliationFromHeadline(self, r):\n",
    "        v = r['subject_str']\n",
    "        p = r['party_str']\n",
    "        if (p =='democrat'):\n",
    "            s2 = set(self.countDemV.get_feature_names())\n",
    "        if (p =='republican'):\n",
    "            s2 = set(self.countRepV.get_feature_names())\n",
    "        if (p != 'democract' and p !='republican'):\n",
    "            return 1 #'true'        \n",
    "        if set(v).intersection(s2):\n",
    "            return 1 #'true'\n",
    "        else:\n",
    "            return 0 #'false'\n",
    "\n",
    "    #API to convert true, mostly-true and half-true to true\n",
    "    # false, barely-true and pants-fire to false\n",
    "    def convertMulticlassToBinaryclass(self, r):\n",
    "        v = r['label']\n",
    "        if (v == 'true'):\n",
    "            return 1 #'true'\n",
    "        if (v == 'mostly-true'):\n",
    "            return 1 #'true'\n",
    "        if (v == 'half-true'):\n",
    "            return 1 #'true'\n",
    "        if (v == 'barely-true'):\n",
    "            return 0 #'false'\n",
    "        if (v == 'false'):\n",
    "            return 0 #'false'\n",
    "        if (v == 'pants-fire'):\n",
    "            return 0 #'false'\n",
    "            \n",
    "            \n",
    "            \n",
    "    def plot_confusion_matrix(self, cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "\n",
    "        plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "        plt.title(title)\n",
    "        plt.colorbar()\n",
    "        tick_marks = np.arange(len(classes))\n",
    "        plt.xticks(tick_marks, classes, rotation=45)\n",
    "        plt.yticks(tick_marks, classes)\n",
    "\n",
    "        if normalize:\n",
    "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "            print(\"Normalized confusion matrix\")\n",
    "        else:\n",
    "            print('Confusion matrix, without normalization')\n",
    "\n",
    "        thresh = cm.max() / 2.\n",
    "        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "            plt.text(j, i, cm[i, j],\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')       \n",
    "            \n",
    "    \n",
    "    def __init__(self):        \n",
    "\n",
    "        columnNamesPar = [\"id\", \"label\", \"statement\", \"subject\", \"speaker\", \"speaker_job_title\", \"state_info\", \"party_affiliation\", \"barely_true_counts\", \"false_counts\", \"half_true_counts\", \"mostly_true_counts\", \"pants_on_fire_counts\", \"context\"]\n",
    "        dataTrainPar = pd.read_csv('input_data/dataset/train_education_enrich.tsv', sep='\\t', header=None, names = columnNamesPar)\n",
    "        dataValidatePar = pd.read_csv('input_data/dataset/valid_education_enrich.tsv', sep='\\t', header=None, names = columnNamesPar)\n",
    "        dataTestPar = pd.read_csv('input_data/dataset/test_education_enrich.tsv', sep='\\t', header=None, names = columnNamesPar)\n",
    "        \n",
    "    \n",
    "        # Remove unwanted columns in the dataset\n",
    "        columnsToRemovePar = ['id', 'speaker', 'context','speaker_job_title', 'barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts']\n",
    "        dataTrainPar = dataTrainPar.drop(columns=columnsToRemovePar)\n",
    "        dataValidatePar = dataValidatePar.drop(columns=columnsToRemovePar)\n",
    "        dataTestPar = dataTestPar.drop(columns=columnsToRemovePar)\n",
    "        \n",
    "        # convert the labels to true and false only\n",
    "        dataTrainPar['label'] = dataTrainPar.apply(self.convertMulticlassToBinaryclass, axis=1)\n",
    "        dataValidatePar['label'] = dataValidatePar.apply(self.convertMulticlassToBinaryclass, axis=1)\n",
    "        dataTestPar['label'] = dataTestPar.apply(self.convertMulticlassToBinaryclass, axis=1)\n",
    "        \n",
    "        # display all the party affiliations and show the count of each party \n",
    "#         dataTrainPar.groupby('party_affiliation').count()[['state_info']].rename(\n",
    "#         columns={'state_info': 'count'}).sort_values(\n",
    "#         'count', ascending=False).reset_index().plot.bar(\n",
    "#         x='party_affiliation', y='count', figsize=(16, 10), fontsize=18);\n",
    "        \n",
    "        # As we are considering only democrat, republican and none (top 3 party affiliations),\n",
    "        # ignoring other party affiliations\n",
    "        rowsToRemove = ['Moderate', 'activist', 'business-leader', 'columnist', 'constitution-party', 'democratic-farmer-labor', 'education-official', 'government-body', 'green', 'independent', 'journalist', 'labor-leader', 'liberal-party-canada', 'libertarian', 'nan', 'newsmaker', 'ocean-state-tea-party-action', 'organization', 'state-official', 'talk-show-host', 'tea-party-member']\n",
    "\n",
    "        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'Moderate']\n",
    "        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'activist']\n",
    "        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'business-leader']\n",
    "        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'columnist']\n",
    "        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'constitution-party']\n",
    "        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'democratic-farmer-labor']\n",
    "        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'education-official']\n",
    "        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'government-body']\n",
    "        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'green']\n",
    "        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'independent']\n",
    "        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'journalist']\n",
    "        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'labor-leader']\n",
    "        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'liberal-party-canada']\n",
    "        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'libertarian']\n",
    "        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'nan']\n",
    "        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'newsmaker']\n",
    "        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'ocean-state-tea-party-action']\n",
    "        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'organization']\n",
    "        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'state-official']\n",
    "        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'talk-show-host']\n",
    "        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'tea-party-member']\n",
    "\n",
    "        # As we are considering only democrat, republican and none (top 3 party affiliations),\n",
    "        # ignoring other party affiliations\n",
    "\n",
    "        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'Moderate']\n",
    "        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'activist']\n",
    "        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'business-leader']\n",
    "        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'columnist']\n",
    "        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'constitution-party']\n",
    "        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'democratic-farmer-labor']\n",
    "        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'education-official']\n",
    "        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'government-body']\n",
    "        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'green']\n",
    "        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'independent']\n",
    "        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'journalist']\n",
    "        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'labor-leader']\n",
    "        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'liberal-party-canada']\n",
    "        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'libertarian']\n",
    "        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'nan']\n",
    "        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'newsmaker']\n",
    "        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'ocean-state-tea-party-action']\n",
    "        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'organization']\n",
    "        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'state-official']\n",
    "        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'talk-show-host']\n",
    "        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'tea-party-member']\n",
    "\n",
    "        \n",
    "        dataTrainPar['party_str'] = dataTrainPar['party_affiliation'].astype(str)\n",
    "        dataTestPar['party_str'] = dataTestPar['party_affiliation'].astype(str)\n",
    "        \n",
    "\n",
    "        #predicting truth level\n",
    "#        dataTrainPar.groupby('label').count()[['party_affiliation']].reset_index().plot.bar(x='label', y='party_affiliation')\n",
    "        \n",
    "        # get the most used democrat words\n",
    "        self.countDemV = CountVectorizer(stop_words='english', min_df=40, max_df=80, token_pattern=u'(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b')\n",
    "\n",
    "        dataTrainDem= dataTrainPar\n",
    "        dataTrainDem = dataTrainPar.loc[dataTrainPar['party_str'] == 'democrat']\n",
    "        dem_count = self.countDemV.fit_transform(dataTrainDem['statement'].values)\n",
    "        \n",
    "        #get the republican most used words\n",
    "        \n",
    "        self.countRepV = CountVectorizer(stop_words='english', min_df=20, max_df=40, token_pattern=u'(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b')\n",
    "        dataTrainRep= dataTrainPar\n",
    "        dataTrainRep = dataTrainPar.loc[dataTrainPar['party_str'] == 'republican']\n",
    "        rep_count = self.countRepV.fit_transform(dataTrainRep['statement'].values)\n",
    "\n",
    "        dataTestDem= dataTestPar\n",
    "        dataTestDem = dataTestPar.loc[dataTestPar['party_str'] == 'democrat']\n",
    "        \n",
    "        dataTrainPar['subject_str'] = dataTrainPar['subject'].astype(str).str.split() \n",
    "        dataTrainPar['label_str'] = dataTrainPar.apply(self.partyAffiliationFromHeadline, axis=1)\n",
    "\n",
    "        dataTestPar['subject_str'] = dataTestPar['subject'].astype(str).str.split() \n",
    "        dataTestPar['label_str'] = dataTestPar.apply(self.partyAffiliationFromHeadline, axis=1)\n",
    "\n",
    "        dataTrainDem['subject_str'] = dataTrainDem['subject'].astype(str).str.split() \n",
    "        dataTrainDem['label_str'] = dataTrainDem.apply(self.partyAffiliationFromHeadline, axis=1)\n",
    "    \n",
    "        dataTestDem['subject_str'] = dataTestDem['subject'].astype(str).str.split() \n",
    "        dataTestDem['label_str'] = dataTestDem.apply(self.partyAffiliationFromHeadline, axis=1)\n",
    "        \n",
    "        \n",
    "        self.model = LogisticRegression()\n",
    "        self.model = self.model.fit(dataTrainPar['label_str'].values.reshape(-1, 1), dataTrainPar['label'].values)\n",
    "        predicted_LogR = self.model.predict(dataTestPar['label_str'].values.reshape(-1, 1))\n",
    "        score = metrics.accuracy_score(dataTestPar['label'], predicted_LogR)\n",
    "        print(\"Party Affiliation Model Trained - accuracy:   %0.6f\" % score)\n",
    "\n",
    "    \n",
    "    def predict(self, headline, party):\n",
    "                \n",
    "        #creating the dataframe with our text so we can leverage the existing code\n",
    "        dfrme = pd.DataFrame(index=[0], columns=['subject', 'party_str'])\n",
    "        dfrme['subject_str'] = headline\n",
    "        dfrme['party_str'] = party        \n",
    "\n",
    "        dfrme['subject'] = headline\n",
    "        dfrme['subject_str'] = dfrme['subject'].astype(str).str.split() \n",
    "        dfrme['label_str'] = dfrme.apply(self.partyAffiliationFromHeadline, axis=1)\n",
    "        \n",
    "        x = dfrme['label_str'].values.reshape(-1, 1)\n",
    "        predicted = self.model.predict(x)\n",
    "        predicedProb = self.model.predict_proba(x)[:,1]\n",
    "        return predicted, predicedProb\n",
    "                    \n",
    "    \n",
    "##testing code\n",
    "f = PartyAffiliation()\n",
    "#pf.predict(\"Says the Annies List political group supports third-trimester abortions on demand\", \"republican\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ipynb.fs.full.m_partyaffiliation import PartyAffiliation\n",
    "partyAffiliation = PartyAffiliation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DATAMINERS_getPartyAffiliationScore(headline, partyName): # return between 0 and 1, being 0 = True,  1 = Fake\n",
    "    if ( (headline == \"\") | (partyName == \"\") ):\n",
    "        return 0\n",
    "    binaryValue, probValue = partyAffiliation.predict(headline, partyName)\n",
    "    return (1 - float(probValue))\n",
    "\n",
    "print(DATAMINERS_getPartyAffiliationScore(\"Says the Annies List political group supports third-trimester abortions on demand\", \"republican\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature 5: Click Bait "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enriched the current clickbait dataset with an additional 3M headlines that are from the enquirer.com who is known for their catchy(clickbait) headlines.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClickbaitOriginalData():\n",
    "    \n",
    "    question_words = ['who', 'whos', 'whose', 'what', 'whats', 'whatre', 'when', 'whenre', 'whens', 'couldnt',\n",
    "            'where', 'wheres', 'whered', 'why', 'whys', 'can', 'cant', 'could', 'will', 'would', 'is',\n",
    "            'isnt', 'should', 'shouldnt', 'you', 'your', 'youre', 'youll', 'youd', 'here', 'heres',\n",
    "            'how', 'hows', 'howd', 'this', 'are', 'arent', 'which', 'does', 'doesnt']\n",
    "\n",
    "    contractions = ['tis', 'aint', 'amnt', 'arent', 'cant', 'couldve', 'couldnt', 'couldntve',\n",
    "                    'didnt', 'doesnt', 'dont', 'gonna', 'gotta', 'hadnt', 'hadntve', 'hasnt',\n",
    "                    'havent', 'hed', 'hednt', 'hedve', 'hell', 'hes', 'hesnt', 'howd', 'howll',\n",
    "                    'hows', 'id', 'idnt', 'idntve', 'idve', 'ill', 'im', 'ive', 'ivent', 'isnt',\n",
    "                    'itd', 'itdnt', 'itdntve', 'itdve', 'itll', 'its', 'itsnt', 'mightnt',\n",
    "                    'mightve', 'mustnt', 'mustntve', 'mustve', 'neednt', 'oclock', 'ol', 'oughtnt',\n",
    "                    'shant', 'shed', 'shednt', 'shedntve', 'shedve', 'shell', 'shes', 'shouldve',\n",
    "                    'shouldnt', 'shouldntve', 'somebodydve', 'somebodydntve', 'somebodys',\n",
    "                    'someoned', 'someonednt', 'someonedntve', 'someonedve', 'someonell', 'someones',\n",
    "                    'somethingd', 'somethingdnt', 'somethingdntve', 'somethingdve', 'somethingll',\n",
    "                    'somethings', 'thatll', 'thats', 'thatd', 'thered', 'therednt', 'theredntve',\n",
    "                    'theredve', 'therere', 'theres', 'theyd', 'theydnt', 'theydntve', 'theydve',\n",
    "                    'theydvent', 'theyll', 'theyontve', 'theyre', 'theyve', 'theyvent', 'wasnt',\n",
    "                    'wed', 'wedve', 'wednt', 'wedntve', 'well', 'wontve', 'were', 'weve', 'werent',\n",
    "                    'whatd', 'whatll', 'whatre', 'whats', 'whatve', 'whens', 'whered', 'wheres',\n",
    "                    'whereve', 'whod', 'whodve', 'wholl', 'whore', 'whos', 'whove', 'whyd', 'whyre',\n",
    "                    'whys', 'wont', 'wontve', 'wouldve', 'wouldnt', 'wouldntve', 'yall', 'yalldve',\n",
    "                    'yalldntve', 'yallll', 'yallont', 'yallllve', 'yallre', 'yallllvent', 'yaint',\n",
    "                    'youd', 'youdve', 'youll', 'youre', 'yourent', 'youve', 'youvent']\n",
    "    \n",
    "    def process_text(self, text):\n",
    "        result = text.replace('/', '').replace('\\n', '')\n",
    "        result = re.sub(r'[1-9]+', 'number', result)\n",
    "        result = re.sub(r'(\\w)(\\1{2,})', r'\\1', result)\n",
    "        result = re.sub(r'(?x)\\b(?=\\w*\\d)\\w+\\s*', '', result)\n",
    "        result = ''.join(t for t in result if t not in punctuation)\n",
    "        result = re.sub(r' +', ' ', result).lower().strip()\n",
    "        return result\n",
    "    \n",
    "    def cnt_stop_words(self, text):\n",
    "        s = text.split()\n",
    "        num = len([word for word in s if word in self.stop])\n",
    "        return num\n",
    "\n",
    "    def num_contract(self, text):\n",
    "        s = text.split()\n",
    "        num = len([word for word in s if word in self.contractions])\n",
    "        return num\n",
    "\n",
    "    def question_word(self, text):\n",
    "        s = text.split()\n",
    "        if s[0] in self.question_words:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def part_of_speech(self, text):\n",
    "        s = text.split()\n",
    "        nonstop = [word for word in s if word not in self.stop]\n",
    "        pos = [part[1] for part in nltk.pos_tag(nonstop)]\n",
    "        pos = ' '.join(pos)\n",
    "        return pos\n",
    "\n",
    "\n",
    "    def __init__(self):        \n",
    "        df_ycb = pd.read_csv('input_data/clickbait/clickbait_data.txt', sep=\"\\n\", header=None, names=['text'])\n",
    "        df_ycb['clickbait'] = 1\n",
    "\n",
    "        df_ncb = pd.read_csv('input_data/clickbait/non_clickbait_data.txt', sep=\"\\n\", header=None, names=['text'])\n",
    "        df_ncb['clickbait'] = 0\n",
    "\n",
    "        df = df_ycb.append(df_ncb, ignore_index=True).reset_index(drop=True)\n",
    "\n",
    "        \n",
    "\n",
    "       \n",
    "        self.stop = stopwords.words('english')\n",
    "       \n",
    "        # Creating some latent variables from the data\n",
    "        df['text']     = df['text'].apply(self.process_text)\n",
    "        df['question'] = df['text'].apply(self.question_word)\n",
    "\n",
    "        df['num_words']       = df['text'].apply(lambda x: len(x.split()))\n",
    "        df['part_speech']     = df['text'].apply(self.part_of_speech)\n",
    "        df['num_contract']    = df['text'].apply(self.num_contract)\n",
    "        df['num_stop_words']  = df['text'].apply(self.cnt_stop_words)\n",
    "        df['stop_word_ratio'] = df['num_stop_words']/df['num_words']\n",
    "        df['contract_ratio']  = df['num_contract']/df['num_words']\n",
    "\n",
    "        # Data visualization functions\n",
    "        figure, axes = plt.subplots(nrows=2, ncols=2, figsize=(12,12))\n",
    "\n",
    "        plot = df.groupby('question')['clickbait'].value_counts().unstack().plot.bar(ax=axes[0,0], rot=0)\n",
    "        plot.set_xlabel('Headline Question')\n",
    "        plot.set_ylabel('Number of Headlines')\n",
    "\n",
    "        plot = df.groupby('num_words')['clickbait'].value_counts().unstack().plot.bar(ax=axes[0,1], rot=0)\n",
    "        plot.set_xlabel('Number of Words')\n",
    "        plot.set_ylabel('Number of Headlines')\n",
    "\n",
    "        plot = df.groupby('num_stop_words')['clickbait'].value_counts().unstack().plot.bar(ax=axes[1,0], rot=0)\n",
    "        plot.set_xlabel('Number of Stop Words')\n",
    "        plot.set_ylabel('Number of Headlines')\n",
    "\n",
    "        plot = df.groupby('num_contract')['clickbait'].value_counts().unstack().plot.bar(ax=axes[1,1], rot=0)\n",
    "        plot.set_xlabel('Number of Contractions')\n",
    "        plot.set_ylabel('Number of Headlines')\n",
    "        \n",
    "        figure, axes = plt.subplots(figsize=(10,8))\n",
    "        sns.heatmap(df.drop(['text','clickbait'], axis=1).corr(), annot=True, vmax=1, linewidths=.5, cmap='Reds')\n",
    "        plt.xticks(rotation=90)\n",
    "        \n",
    "        \n",
    "        # continuation of original code\n",
    "        df.drop(['num_stop_words','num_contract'], axis=1, inplace=True)\n",
    "\n",
    "        df_train, df_test = train_test_split(df, test_size=0.2, random_state=0)\n",
    "\n",
    "        self.tfidf = TfidfVectorizer(min_df=3, max_features=None, strip_accents='unicode',\n",
    "                                   analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1,5),\n",
    "                                   use_idf=1, smooth_idf=1, sublinear_tf=1)\n",
    "\n",
    "        X_train_text = self.tfidf.fit_transform(df_train['text'])\n",
    "        X_test_text  = self.tfidf.transform(df_test['text'])\n",
    "\n",
    "        self.cvec = CountVectorizer()\n",
    "\n",
    "        X_train_pos = self.cvec.fit_transform(df_train['part_speech'])\n",
    "        X_test_pos  = self.cvec.transform(df_test['part_speech'])\n",
    "\n",
    "        self.scNoMean = StandardScaler(with_mean=False)  # we pass with_mean=False to preserve the sparse matrix\n",
    "        X_train_pos_sc = self.scNoMean.fit_transform(X_train_pos)\n",
    "        X_test_pos_sc  = self.scNoMean.transform(X_test_pos)\n",
    "\n",
    "        X_train_val = df_train.drop(['clickbait','text','part_speech'], axis=1).values\n",
    "        X_test_val  = df_test.drop(['clickbait','text','part_speech'], axis=1).values\n",
    "\n",
    "        self.sc = StandardScaler()\n",
    "        X_train_val_sc = self.sc.fit(X_train_val).transform(X_train_val)\n",
    "        X_test_val_sc  = self.sc.transform(X_test_val)\n",
    "\n",
    "        y_train = df_train['clickbait'].values\n",
    "        y_test  = df_test['clickbait'].values\n",
    "\n",
    "\n",
    "        #print logistic regression and model accuracy \n",
    "        X_train = sparse.hstack([X_train_val_sc, X_train_text, X_train_pos_sc]).tocsr()\n",
    "        X_test  = sparse.hstack([X_test_val_sc, X_test_text, X_test_pos_sc]).tocsr()\n",
    "\n",
    "        self.model = LogisticRegression(penalty='l2', C=98.94736842105263)\n",
    "        self.model = self.model.fit(X_train, y_train)\n",
    "        \n",
    "        predicted_LogR = self.model.predict(X_test)\n",
    "        score = metrics.accuracy_score(y_test, predicted_LogR)\n",
    "        print(\"Clickbait Model Trained - accuracy:   %0.6f\" % score)\n",
    "        \n",
    "\n",
    "        # optimizing logistic regression\n",
    "        param_grid = [{'C': np.linspace(90,100,20)}]\n",
    "\n",
    "        grid_cv = GridSearchCV(LogisticRegression(), param_grid, scoring='accuracy', cv=5, verbose=1)\n",
    "        grid_cv.fit(X_train, y_train)\n",
    "\n",
    "        print(grid_cv.best_params_)\n",
    "        print(grid_cv.best_score_)\n",
    "\n",
    "        # running cross validation\n",
    "        model = LogisticRegression(penalty='l2', C=93.684210526315795)\n",
    "        model = model.fit(X_train, y_train)\n",
    "        predict = model.predict(X_test)\n",
    "\n",
    "        print(classification_report(y_test, predict))\n",
    "        \n",
    "        #Confustion matrix\n",
    "        figure, axes = plt.subplots(figsize=(8,6))\n",
    "        cm = confusion_matrix(y_test, predict).T\n",
    "        cm = cm.astype('float')/cm.sum(axis=0)\n",
    "\n",
    "        sns.heatmap(cm, annot=True, cmap='Blues');\n",
    "        plt.xlabel('True Label')\n",
    "        plt.ylabel('Predicted Label')\n",
    "        \n",
    "        #applying roc to visualize logistic regression \n",
    "        figure, axes = plt.subplots(figsize=(8,8))\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, model.predict_proba(X_test)[:,1])\n",
    "        roc_auc = roc_auc_score(y_test, predict)\n",
    "\n",
    "        plt.plot(fpr, tpr, lw=1, label='AUC = %0.2f'%(roc_auc))\n",
    "        plt.plot([0, 1], [0, 1], '--k', lw=1)\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('ROC')\n",
    "        plt.legend(loc=\"lower right\", frameon = True).get_frame().set_edgecolor('black')\n",
    "        \n",
    "        # tdidf to print most common words \n",
    "        tfidf_cb = TfidfVectorizer(min_df=3, max_features=None, strip_accents='unicode',\n",
    "                           analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1,3),\n",
    "                           use_idf=1, smooth_idf=1, sublinear_tf=1, stop_words='english')\n",
    "        cb = tfidf_cb.fit_transform(df_train.loc[df['clickbait']==1, 'text'])\n",
    "\n",
    "        tfidf_ncb = TfidfVectorizer(min_df=3, max_features=None, strip_accents='unicode',\n",
    "                                   analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1,3),\n",
    "                                   use_idf=1, smooth_idf=1, sublinear_tf=1, stop_words='english')\n",
    "        ncb = tfidf_ncb.fit_transform(df_train.loc[df['clickbait']==0, 'text'])\n",
    "\n",
    "        cb_values = cb.mean(axis=0).tolist()\n",
    "        cb_names = tfidf_cb.get_feature_names()\n",
    "        ncb_values = ncb.mean(axis=0).tolist()\n",
    "        ncb_names = tfidf_ncb.get_feature_names()\n",
    "\n",
    "        q_cb = pd.DataFrame()\n",
    "        q_cb['names'] = cb_names\n",
    "        q_cb['values'] = list(itertools.chain.from_iterable(cb_values))\n",
    "        q_cb = q_cb.sort_values('values', ascending=True)\n",
    "\n",
    "        q_ncb = pd.DataFrame()\n",
    "        q_ncb['names'] = ncb_names\n",
    "        q_ncb['values'] = list(itertools.chain.from_iterable(ncb_values))\n",
    "        q_ncb = q_ncb.sort_values('values', ascending=True)\n",
    "        \n",
    "        # graphing the non click bait words vs click bait words\n",
    "        figure, axes = plt.subplots(nrows=1, ncols=2, figsize=(12,7))\n",
    "        plt.tight_layout(8,1)\n",
    "\n",
    "        plot = q_cb[-20:].plot.barh(x='names', y='values', ax=axes[0], rot=0)\n",
    "        plot.set_xlabel('Mean tf-idf')\n",
    "        plot.set_ylabel('Words')\n",
    "        plot.set_title('Clickbait')\n",
    "\n",
    "        plot = q_ncb[-20:].plot.barh(x='names', y='values', ax=axes[1], rot=0)\n",
    "        plot.set_xlabel('Mean tf-idf')\n",
    "        plot.set_ylabel('Words')\n",
    "        plot.set_title('Non-Clickbait')\n",
    "\n",
    "\n",
    "#     predict = model.predict(X_test)\n",
    "#     print(classification_report(y_test, predict))\n",
    "\n",
    "\n",
    "    def predict(self, text):\n",
    "        #creating the dataframe with our text so we can leverage the existing code\n",
    "        dfrme = pd.DataFrame(index=[0], columns=['text'])\n",
    "        dfrme['text'] = text\n",
    "\n",
    "        #processing text\n",
    "        dfrme['text']     = dfrme['text'].apply(self.process_text)\n",
    "\n",
    "        #adding latent variables\n",
    "        dfrme['question'] = dfrme['text'].apply(self.question_word)\n",
    "        dfrme['num_words']       = dfrme['text'].apply(lambda x: len(x.split()))\n",
    "        dfrme['part_speech']     = dfrme['text'].apply(self.part_of_speech)\n",
    "        dfrme['num_contract']    = dfrme['text'].apply(self.num_contract)\n",
    "        dfrme['num_stop_words']  = dfrme['text'].apply(self.cnt_stop_words)\n",
    "        dfrme['stop_word_ratio'] = dfrme['num_stop_words']/dfrme['num_words']\n",
    "        dfrme['contract_ratio']  = dfrme['num_contract']/dfrme['num_words']\n",
    "\n",
    "        #removing latent variables that have high colinearity with other features\n",
    "        dfrme.drop(['num_stop_words','num_contract'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "        Xtxt_val  = dfrme.drop(['text','part_speech'], axis=1).values\n",
    "        Xtxt_val_sc  = self.sc.transform(Xtxt_val)\n",
    "\n",
    "        Xtxt_text  = self.tfidf.transform(dfrme['text'])\n",
    "\n",
    "        Xtxt_pos  = self.cvec.transform(dfrme['part_speech'])\n",
    "        Xtxt_pos_sc  = self.scNoMean.transform(Xtxt_pos)\n",
    "        Xtxt  = sparse.hstack([Xtxt_val_sc, Xtxt_text, Xtxt_pos_sc]).tocsr()\n",
    "\n",
    "        predicted = self.model.predict(Xtxt)\n",
    "        predicedProb = self.model.predict_proba(Xtxt)[:,1]\n",
    "        return predicted, predicedProb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ipynb.fs.full.m_clickbait import Clickbait\n",
    "clickBait = ClickbaitOriginalData()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClickbaitNewData():\n",
    "    \n",
    "    question_words = ['who', 'whos', 'whose', 'what', 'whats', 'whatre', 'when', 'whenre', 'whens', 'couldnt',\n",
    "            'where', 'wheres', 'whered', 'why', 'whys', 'can', 'cant', 'could', 'will', 'would', 'is',\n",
    "            'isnt', 'should', 'shouldnt', 'you', 'your', 'youre', 'youll', 'youd', 'here', 'heres',\n",
    "            'how', 'hows', 'howd', 'this', 'are', 'arent', 'which', 'does', 'doesnt']\n",
    "\n",
    "    contractions = ['tis', 'aint', 'amnt', 'arent', 'cant', 'couldve', 'couldnt', 'couldntve',\n",
    "                    'didnt', 'doesnt', 'dont', 'gonna', 'gotta', 'hadnt', 'hadntve', 'hasnt',\n",
    "                    'havent', 'hed', 'hednt', 'hedve', 'hell', 'hes', 'hesnt', 'howd', 'howll',\n",
    "                    'hows', 'id', 'idnt', 'idntve', 'idve', 'ill', 'im', 'ive', 'ivent', 'isnt',\n",
    "                    'itd', 'itdnt', 'itdntve', 'itdve', 'itll', 'its', 'itsnt', 'mightnt',\n",
    "                    'mightve', 'mustnt', 'mustntve', 'mustve', 'neednt', 'oclock', 'ol', 'oughtnt',\n",
    "                    'shant', 'shed', 'shednt', 'shedntve', 'shedve', 'shell', 'shes', 'shouldve',\n",
    "                    'shouldnt', 'shouldntve', 'somebodydve', 'somebodydntve', 'somebodys',\n",
    "                    'someoned', 'someonednt', 'someonedntve', 'someonedve', 'someonell', 'someones',\n",
    "                    'somethingd', 'somethingdnt', 'somethingdntve', 'somethingdve', 'somethingll',\n",
    "                    'somethings', 'thatll', 'thats', 'thatd', 'thered', 'therednt', 'theredntve',\n",
    "                    'theredve', 'therere', 'theres', 'theyd', 'theydnt', 'theydntve', 'theydve',\n",
    "                    'theydvent', 'theyll', 'theyontve', 'theyre', 'theyve', 'theyvent', 'wasnt',\n",
    "                    'wed', 'wedve', 'wednt', 'wedntve', 'well', 'wontve', 'were', 'weve', 'werent',\n",
    "                    'whatd', 'whatll', 'whatre', 'whats', 'whatve', 'whens', 'whered', 'wheres',\n",
    "                    'whereve', 'whod', 'whodve', 'wholl', 'whore', 'whos', 'whove', 'whyd', 'whyre',\n",
    "                    'whys', 'wont', 'wontve', 'wouldve', 'wouldnt', 'wouldntve', 'yall', 'yalldve',\n",
    "                    'yalldntve', 'yallll', 'yallont', 'yallllve', 'yallre', 'yallllvent', 'yaint',\n",
    "                    'youd', 'youdve', 'youll', 'youre', 'yourent', 'youve', 'youvent']\n",
    "    \n",
    "    def process_text(self, text):\n",
    "        result = text.replace('/', '').replace('\\n', '')\n",
    "        result = re.sub(r'[1-9]+', 'number', result)\n",
    "        result = re.sub(r'(\\w)(\\1{2,})', r'\\1', result)\n",
    "        result = re.sub(r'(?x)\\b(?=\\w*\\d)\\w+\\s*', '', result)\n",
    "        result = ''.join(t for t in result if t not in punctuation)\n",
    "        result = re.sub(r' +', ' ', result).lower().strip()\n",
    "        return result\n",
    "    \n",
    "    def cnt_stop_words(self, text):\n",
    "        s = text.split()\n",
    "        num = len([word for word in s if word in self.stop])\n",
    "        return num\n",
    "\n",
    "    def num_contract(self, text):\n",
    "        s = text.split()\n",
    "        num = len([word for word in s if word in self.contractions])\n",
    "        return num\n",
    "\n",
    "    def question_word(self, text):\n",
    "        s = text.split()\n",
    "        if s[0] in self.question_words:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def part_of_speech(self, text):\n",
    "        s = text.split()\n",
    "        nonstop = [word for word in s if word not in self.stop]\n",
    "        pos = [part[1] for part in nltk.pos_tag(nonstop)]\n",
    "        pos = ' '.join(pos)\n",
    "        return pos\n",
    "\n",
    "\n",
    "    def __init__(self):        \n",
    "        #df_ycb = pd.read_csv('input_data/clickbait/examiner-date-text.txt', sep=\"\\n\", header=None, names=['text'])\n",
    "        df_ycb = pd.read_csv('input_data/clickbait/clickbait_data.txt', sep=\"\\n\", header=None, names=['text'])\n",
    "        df_ycb['clickbait'] = 1\n",
    "\n",
    "        df_ncb = pd.read_csv('input_data/clickbait/non_clickbait_data.txt', sep=\"\\n\", header=None, names=['text'])\n",
    "        df_ncb['clickbait'] = 0\n",
    "\n",
    "        df = df_ycb.append(df_ncb, ignore_index=True).reset_index(drop=True)\n",
    "\n",
    "        \n",
    "\n",
    "       \n",
    "        self.stop = stopwords.words('english')\n",
    "       \n",
    "        # Creating some latent variables from the data\n",
    "        df['text']     = df['text'].apply(self.process_text)\n",
    "        df['question'] = df['text'].apply(self.question_word)\n",
    "\n",
    "        df['num_words']       = df['text'].apply(lambda x: len(x.split()))\n",
    "        df['part_speech']     = df['text'].apply(self.part_of_speech)\n",
    "        df['num_contract']    = df['text'].apply(self.num_contract)\n",
    "        df['num_stop_words']  = df['text'].apply(self.cnt_stop_words)\n",
    "        df['stop_word_ratio'] = df['num_stop_words']/df['num_words']\n",
    "        df['contract_ratio']  = df['num_contract']/df['num_words']\n",
    "\n",
    "        # Data visualization functions\n",
    "        figure, axes = plt.subplots(nrows=2, ncols=2, figsize=(12,12))\n",
    "\n",
    "        plot = df.groupby('question')['clickbait'].value_counts().unstack().plot.bar(ax=axes[0,0], rot=0)\n",
    "        plot.set_xlabel('Headline Question')\n",
    "        plot.set_ylabel('Number of Headlines')\n",
    "\n",
    "        plot = df.groupby('num_words')['clickbait'].value_counts().unstack().plot.bar(ax=axes[0,1], rot=0)\n",
    "        plot.set_xlabel('Number of Words')\n",
    "        plot.set_ylabel('Number of Headlines')\n",
    "\n",
    "        plot = df.groupby('num_stop_words')['clickbait'].value_counts().unstack().plot.bar(ax=axes[1,0], rot=0)\n",
    "        plot.set_xlabel('Number of Stop Words')\n",
    "        plot.set_ylabel('Number of Headlines')\n",
    "\n",
    "        plot = df.groupby('num_contract')['clickbait'].value_counts().unstack().plot.bar(ax=axes[1,1], rot=0)\n",
    "        plot.set_xlabel('Number of Contractions')\n",
    "        plot.set_ylabel('Number of Headlines')\n",
    "        \n",
    "        figure, axes = plt.subplots(figsize=(10,8))\n",
    "        sns.heatmap(df.drop(['text','clickbait'], axis=1).corr(), annot=True, vmax=1, linewidths=.5, cmap='Reds')\n",
    "        plt.xticks(rotation=90)\n",
    "        \n",
    "        \n",
    "        # continuation of original code\n",
    "        df.drop(['num_stop_words','num_contract'], axis=1, inplace=True)\n",
    "\n",
    "        df_train, df_test = train_test_split(df, test_size=0.2, random_state=0)\n",
    "\n",
    "        self.tfidf = TfidfVectorizer(min_df=3, max_features=None, strip_accents='unicode',\n",
    "                                   analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1,5),\n",
    "                                   use_idf=1, smooth_idf=1, sublinear_tf=1)\n",
    "\n",
    "        X_train_text = self.tfidf.fit_transform(df_train['text'])\n",
    "        X_test_text  = self.tfidf.transform(df_test['text'])\n",
    "\n",
    "        self.cvec = CountVectorizer()\n",
    "\n",
    "        X_train_pos = self.cvec.fit_transform(df_train['part_speech'])\n",
    "        X_test_pos  = self.cvec.transform(df_test['part_speech'])\n",
    "\n",
    "        self.scNoMean = StandardScaler(with_mean=False)  # we pass with_mean=False to preserve the sparse matrix\n",
    "        X_train_pos_sc = self.scNoMean.fit_transform(X_train_pos)\n",
    "        X_test_pos_sc  = self.scNoMean.transform(X_test_pos)\n",
    "\n",
    "        X_train_val = df_train.drop(['clickbait','text','part_speech'], axis=1).values\n",
    "        X_test_val  = df_test.drop(['clickbait','text','part_speech'], axis=1).values\n",
    "\n",
    "        self.sc = StandardScaler()\n",
    "        X_train_val_sc = self.sc.fit(X_train_val).transform(X_train_val)\n",
    "        X_test_val_sc  = self.sc.transform(X_test_val)\n",
    "\n",
    "        y_train = df_train['clickbait'].values\n",
    "        y_test  = df_test['clickbait'].values\n",
    "\n",
    "\n",
    "        #print logistic regression and model accuracy \n",
    "        X_train = sparse.hstack([X_train_val_sc, X_train_text, X_train_pos_sc]).tocsr()\n",
    "        X_test  = sparse.hstack([X_test_val_sc, X_test_text, X_test_pos_sc]).tocsr()\n",
    "\n",
    "        self.model = LogisticRegression(penalty='l2', C=95)\n",
    "        self.model = self.model.fit(X_train, y_train)\n",
    "        \n",
    "        predicted_LogR = self.model.predict(X_test)\n",
    "        score = metrics.accuracy_score(y_test, predicted_LogR)\n",
    "        print(\"Clickbait Model Trained - accuracy:   %0.6f\" % score)\n",
    "        \n",
    "\n",
    "        # optimizing logistic regression\n",
    "        param_grid = [{'C': np.linspace(90,100,20)}]\n",
    "\n",
    "        grid_cv = GridSearchCV(LogisticRegression(), param_grid, scoring='accuracy', cv=5, verbose=1)\n",
    "        grid_cv.fit(X_train, y_train)\n",
    "\n",
    "        print(grid_cv.best_params_)\n",
    "        print(grid_cv.best_score_)\n",
    "\n",
    "        # running cross validation\n",
    "        model = LogisticRegression(penalty='l2', C=93)\n",
    "        model = model.fit(X_train, y_train)\n",
    "        predict = model.predict(X_test)\n",
    "\n",
    "        print(classification_report(y_test, predict))\n",
    "        \n",
    "        #Confustion matrix\n",
    "        figure, axes = plt.subplots(figsize=(8,6))\n",
    "        cm = confusion_matrix(y_test, predict).T\n",
    "        cm = cm.astype('float')/cm.sum(axis=0)\n",
    "\n",
    "        sns.heatmap(cm, annot=True, cmap='Blues');\n",
    "        plt.xlabel('True Label')\n",
    "        plt.ylabel('Predicted Label')\n",
    "        \n",
    "        #applying roc to visualize logistic regression \n",
    "        figure, axes = plt.subplots(figsize=(8,8))\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, model.predict_proba(X_test)[:,1])\n",
    "        roc_auc = roc_auc_score(y_test, predict)\n",
    "\n",
    "        plt.plot(fpr, tpr, lw=1, label='AUC = %0.2f'%(roc_auc))\n",
    "        plt.plot([0, 1], [0, 1], '--k', lw=1)\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('ROC')\n",
    "        plt.legend(loc=\"lower right\", frameon = True).get_frame().set_edgecolor('black')\n",
    "        \n",
    "        # tdidf to print most common words \n",
    "        tfidf_cb = TfidfVectorizer(min_df=3, max_features=None, strip_accents='unicode',\n",
    "                           analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1,3),\n",
    "                           use_idf=1, smooth_idf=1, sublinear_tf=1, stop_words='english')\n",
    "        cb = tfidf_cb.fit_transform(df_train.loc[df['clickbait']==1, 'text'])\n",
    "\n",
    "        tfidf_ncb = TfidfVectorizer(min_df=3, max_features=None, strip_accents='unicode',\n",
    "                                   analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1,3),\n",
    "                                   use_idf=1, smooth_idf=1, sublinear_tf=1, stop_words='english')\n",
    "        ncb = tfidf_ncb.fit_transform(df_train.loc[df['clickbait']==0, 'text'])\n",
    "\n",
    "        cb_values = cb.mean(axis=0).tolist()\n",
    "        cb_names = tfidf_cb.get_feature_names()\n",
    "        ncb_values = ncb.mean(axis=0).tolist()\n",
    "        ncb_names = tfidf_ncb.get_feature_names()\n",
    "\n",
    "        q_cb = pd.DataFrame()\n",
    "        q_cb['names'] = cb_names\n",
    "        q_cb['values'] = list(itertools.chain.from_iterable(cb_values))\n",
    "        q_cb = q_cb.sort_values('values', ascending=True)\n",
    "\n",
    "        q_ncb = pd.DataFrame()\n",
    "        q_ncb['names'] = ncb_names\n",
    "        q_ncb['values'] = list(itertools.chain.from_iterable(ncb_values))\n",
    "        q_ncb = q_ncb.sort_values('values', ascending=True)\n",
    "        \n",
    "        # graphing the non click bait words vs click bait words\n",
    "        figure, axes = plt.subplots(nrows=1, ncols=2, figsize=(12,7))\n",
    "        plt.tight_layout(8,1)\n",
    "\n",
    "        plot = q_cb[-20:].plot.barh(x='names', y='values', ax=axes[0], rot=0)\n",
    "        plot.set_xlabel('Mean tf-idf')\n",
    "        plot.set_ylabel('Words')\n",
    "        plot.set_title('Clickbait')\n",
    "\n",
    "        plot = q_ncb[-20:].plot.barh(x='names', y='values', ax=axes[1], rot=0)\n",
    "        plot.set_xlabel('Mean tf-idf')\n",
    "        plot.set_ylabel('Words')\n",
    "        plot.set_title('Non-Clickbait')\n",
    "        \n",
    "        # Apply random forest to clickbait words to see if there is a better classification score. \n",
    "        clf=RandomForestClassifier(n_estimators=100)\n",
    "        clf.fit(X_train,y_train)\n",
    "        y_pred=clf.predict(X_test)\n",
    "        \n",
    "        feature_imp = pd.Series(clf.feature_importances_,).sort_values(ascending=False)\n",
    "        feature_imp\n",
    "        \n",
    "        print(\"Accuracy for random forest model:\",metrics.accuracy_score(y_test, y_pred))\n",
    "        sns.barplot(x=feature_imp, y=feature_imp.index)\n",
    "        # Add labels to your graph\n",
    "        plt.xlabel('Feature Importance Score')\n",
    "        plt.ylabel('Features')\n",
    "        plt.title(\"Visualizing Important Features\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "#     predict = model.predict(X_test)\n",
    "#     print(classification_report(y_test, predict))\n",
    "\n",
    "\n",
    "    def predict(self, text):\n",
    "        #creating the dataframe with our text so we can leverage the existing code\n",
    "        dfrme = pd.DataFrame(index=[0], columns=['text'])\n",
    "        dfrme['text'] = text\n",
    "\n",
    "        #processing text\n",
    "        dfrme['text']     = dfrme['text'].apply(self.process_text)\n",
    "\n",
    "        #adding latent variables\n",
    "        dfrme['question'] = dfrme['text'].apply(self.question_word)\n",
    "        dfrme['num_words']       = dfrme['text'].apply(lambda x: len(x.split()))\n",
    "        dfrme['part_speech']     = dfrme['text'].apply(self.part_of_speech)\n",
    "        dfrme['num_contract']    = dfrme['text'].apply(self.num_contract)\n",
    "        dfrme['num_stop_words']  = dfrme['text'].apply(self.cnt_stop_words)\n",
    "        dfrme['stop_word_ratio'] = dfrme['num_stop_words']/dfrme['num_words']\n",
    "        dfrme['contract_ratio']  = dfrme['num_contract']/dfrme['num_words']\n",
    "\n",
    "        #removing latent variables that have high colinearity with other features\n",
    "        dfrme.drop(['num_stop_words','num_contract'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "        Xtxt_val  = dfrme.drop(['text','part_speech'], axis=1).values\n",
    "        Xtxt_val_sc  = self.sc.transform(Xtxt_val)\n",
    "\n",
    "        Xtxt_text  = self.tfidf.transform(dfrme['text'])\n",
    "\n",
    "        Xtxt_pos  = self.cvec.transform(dfrme['part_speech'])\n",
    "        Xtxt_pos_sc  = self.scNoMean.transform(Xtxt_pos)\n",
    "        Xtxt  = sparse.hstack([Xtxt_val_sc, Xtxt_text, Xtxt_pos_sc]).tocsr()\n",
    "\n",
    "        predicted = self.model.predict(Xtxt)\n",
    "        predicedProb = self.model.predict_proba(Xtxt)[:,1]\n",
    "        return predicted, predicedProb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:56: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:39: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clickbait Model Trained - accuracy:   0.976562\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 98.94736842105263}\n",
      "0.975625\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.98      0.98      3204\n",
      "          1       0.98      0.97      0.98      3196\n",
      "\n",
      "avg / total       0.98      0.98      0.98      6400\n",
      "\n",
      "('Accuracy for random forest model:', 0.9584375)\n"
     ]
    }
   ],
   "source": [
    "clickBaitNewData = ClickbaitNewData()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
