{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import gensim\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse\n",
    "# Code source: https://degravek.github.io/project-pages/project1/2017/04/28/New-Notebook/\n",
    "# Dataset from Chakraborty et al. (https://github.com/bhargaviparanjape/clickbait/tree/master/dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Analysis Model Trained - accuracy:   0.960784\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True, 0.9811315920552556)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import  LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import *\n",
    "from sklearn import metrics\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class SentimentAnalysis():\n",
    "\n",
    "    def __init__(self):        \n",
    "\n",
    "        columnNames = [\"jsonid\", \"label\", \"headline_text\", \"subject\", \"speaker\", \"speaker_job_title\", \"state_info\", \"party_affiliation\", \"barely_true_counts\", \"false_counts\", \"half_true_counts\", \"mostly_true_counts\", \"pants_on_fire_counts\", \"context\",\"clean\", \"sentiment_vector\",\"vader_polarity\", \"sentiment_score\"]\n",
    "        dataTrain = pd.read_csv('input_data/train_sentiment.csv', sep=',', header=None, names = columnNames)\n",
    "        dataTest = pd.read_csv('input_data/test_sentiment.csv', sep=',', header=None, names = columnNames)\n",
    "\n",
    "        #dropping columns\n",
    "        columnsToRemove = ['jsonid', 'label', 'subject', 'speaker','speaker_job_title', 'state_info', 'party_affiliation', 'barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts', 'context', 'sentiment_vector']\n",
    "        dataTrain = dataTrain.drop(columns=columnsToRemove)\n",
    "        dataTest = dataTest.drop(columns=columnsToRemove)\n",
    "        dataTrain = dataTrain.iloc[1:] \n",
    "        dataTest = dataTest.iloc[1:]\n",
    "    \n",
    "    \n",
    "        tfidfV = TfidfVectorizer(stop_words='english', min_df=5, max_df=30, use_idf=True, smooth_idf=True, token_pattern=u'(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b')\n",
    "\n",
    "        '''\n",
    "        self.logR_pipeline = Pipeline([\n",
    "                ('LogRCV', tfidfV),\n",
    "                ('LogR_clf',LogisticRegression(solver='liblinear', C=32/100))\n",
    "                ])\n",
    "        '''\n",
    "        \n",
    "        self.logR_pipeline = Pipeline([\n",
    "                ('LogRCV', tfidfV),\n",
    "                ('LogR_clf',RandomForestClassifier(n_estimators=100, max_depth=8, random_state=0))\n",
    "                ])\n",
    "        \n",
    "\n",
    "        self.logR_pipeline.fit(dataTrain['headline_text'],dataTrain['vader_polarity'])\n",
    "        predicted_LogR = self.logR_pipeline.predict(dataTest['headline_text'])\n",
    "        score = metrics.accuracy_score(dataTest['vader_polarity'], predicted_LogR)\n",
    "        print(\"Sentiment Analysis Model Trained - accuracy:   %0.6f\" % score)\n",
    "        \n",
    "\n",
    "    def predict(self, text):\n",
    "        predicted = self.logR_pipeline.predict([text])\n",
    "        predicedProb = self.logR_pipeline.predict_proba([text])[:,1]\n",
    "        return bool(predicted), float(predicedProb)\n",
    "    \n",
    "    \n",
    "sa = SentimentAnalysis()\n",
    "sa.predict(\"Says the Annies List political group supports third-trimester abortions on demand.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Analysis Model Trained - accuracy:   0.618309\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(False, 0.6379006769704134)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import  LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import *\n",
    "from sklearn import metrics\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class SentimentAnalysis():\n",
    "\n",
    "    def __init__(self):        \n",
    "\n",
    "        columnNames = [\"jsonid\", \"label\", \"headline_text\", \"subject\", \"speaker\", \"speaker_job_title\", \"state_info\", \"party_affiliation\", \"barely_true_counts\", \"false_counts\", \"half_true_counts\", \"mostly_true_counts\", \"pants_on_fire_counts\", \"context\",\"clean\", \"sentiment_vector\",\"vader_polarity\", \"sentiment_score\"]\n",
    "        sentimentTrain = pd.read_csv('input_data/sentiment/my_train_with_raw.csv')\n",
    "        sentimentTest = pd.read_csv('input_data/sentiment/my_test_with_raw.csv')\n",
    "        \n",
    "        train = pd.read_csv('input_data/train_sentiment.csv')\n",
    "        test = pd.read_csv('input_data/test_sentiment.csv')\n",
    "        \n",
    "        train = train.merge(sentimentTrain, how = 'outer')\n",
    "        test = test.merge(sentimentTest, how = 'outer')\n",
    "        \n",
    "        train.head()\n",
    "        test.head()\n",
    "        \n",
    "        #dropping columns\n",
    "        #  = ['jsonid', 'label', 'subject', 'speaker','speaker_job_title', 'state_info', 'party_affiliation', 'barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts', 'context', 'sentiment_vector']\n",
    "        #dataTrain = dataTrain.drop(columns=columnsToRemove)\n",
    "        #dataTest = dataTest.drop(columns=columnsToRemove)\n",
    "        \n",
    "        tfidfV = TfidfVectorizer(stop_words='english', min_df=5, max_df=30, use_idf=True, smooth_idf=True, token_pattern=u'(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b')\n",
    "\n",
    "        self.logR_pipeline = Pipeline([\n",
    "                ('LogRCV', tfidfV),\n",
    "                ('LogR_clf',LogisticRegression(solver='liblinear', C=32/100))\n",
    "                ])\n",
    "        \n",
    "        self.logR_pipeline = Pipeline([\n",
    "                ('LogRCV', tfidfV),\n",
    "                ('LogR_clf',RandomForestClassifier(n_estimators=100, max_depth=5, random_state=0))\n",
    "                ])\n",
    "\n",
    "        self.logR_pipeline.fit(train['headline_text'],train['polarity'])\n",
    "        predicted_LogR = self.logR_pipeline.predict(test['headline_text'])\n",
    "        score = metrics.accuracy_score(test['polarity'], predicted_LogR)\n",
    "        print(\"Sentiment Analysis Model Trained - accuracy:   %0.6f\" % score)\n",
    "        \n",
    "\n",
    "    def predict(self, text):\n",
    "        predicted = self.logR_pipeline.predict([text])\n",
    "        predicedProb = self.logR_pipeline.predict_proba([text])[:,1]\n",
    "        return bool(predicted), float(predicedProb)\n",
    "    \n",
    "    \n",
    "sa = SentimentAnalysis()\n",
    "sa.predict(\"Says the Annies List political group supports third-trimester abortions on demand.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1289, 22)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0_x</th>\n",
       "      <th>jsonid</th>\n",
       "      <th>label</th>\n",
       "      <th>headline_text</th>\n",
       "      <th>subject</th>\n",
       "      <th>speaker</th>\n",
       "      <th>speakerjobtitle</th>\n",
       "      <th>stateinfo</th>\n",
       "      <th>partyaffiliation</th>\n",
       "      <th>barelytruecounts</th>\n",
       "      <th>...</th>\n",
       "      <th>mostlytrueocunts</th>\n",
       "      <th>pantsonfirecounts</th>\n",
       "      <th>context</th>\n",
       "      <th>clean</th>\n",
       "      <th>sentiment_vector</th>\n",
       "      <th>vader_polarity</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>Unnamed: 0_y</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>80.0</td>\n",
       "      <td>2854.json</td>\n",
       "      <td>pants-fire</td>\n",
       "      <td>Rebuilding three high schools will benefit 40 ...</td>\n",
       "      <td>education</td>\n",
       "      <td>carole-smith</td>\n",
       "      <td>Portland schools superintendent</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>a press briefing</td>\n",
       "      <td>rebuild three high school benefit percent port...</td>\n",
       "      <td>[0.0, 0.75, 0.25, 0.4588]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4588</td>\n",
       "      <td>80</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>124.0</td>\n",
       "      <td>10147.json</td>\n",
       "      <td>false</td>\n",
       "      <td>On an income cap for recipients of the popular...</td>\n",
       "      <td>education</td>\n",
       "      <td>jason-carter</td>\n",
       "      <td>State Senator</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>democrat</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>forum</td>\n",
       "      <td>incom cap recipe popular hope scholarship</td>\n",
       "      <td>[0.0, 0.412, 0.588, 0.6908]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6908</td>\n",
       "      <td>124</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>168.0</td>\n",
       "      <td>7744.json</td>\n",
       "      <td>mostly-true</td>\n",
       "      <td>Says states mandated tests come from an Englis...</td>\n",
       "      <td>education</td>\n",
       "      <td>george-lavender</td>\n",
       "      <td>State representative</td>\n",
       "      <td>Texas</td>\n",
       "      <td>republican</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>in Texas House of Representatives debate</td>\n",
       "      <td>say state mandat test come english company</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>168</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>181.0</td>\n",
       "      <td>5470.json</td>\n",
       "      <td>true</td>\n",
       "      <td>Pasco County schools have graduation rates sub...</td>\n",
       "      <td>education</td>\n",
       "      <td>heather-fiorentino</td>\n",
       "      <td>superintendent of Pasco County's public schools</td>\n",
       "      <td>Florida</td>\n",
       "      <td>republican</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>a press release</td>\n",
       "      <td>pisco county school graduate rate substantia h...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>181</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>187.0</td>\n",
       "      <td>8320.json</td>\n",
       "      <td>mostly-true</td>\n",
       "      <td>Today many Florida teachers are at risk of hav...</td>\n",
       "      <td>education</td>\n",
       "      <td>charlie-crist</td>\n",
       "      <td>0</td>\n",
       "      <td>Florida</td>\n",
       "      <td>democrat</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>a Tampa Bay Times op-ed</td>\n",
       "      <td>today mani florida teacher risk pay impact per...</td>\n",
       "      <td>[0.241, 0.759, 0.0, -0.3612]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.3612</td>\n",
       "      <td>187</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0_x      jsonid        label  \\\n",
       "0          80.0   2854.json   pants-fire   \n",
       "1         124.0  10147.json        false   \n",
       "2         168.0   7744.json  mostly-true   \n",
       "3         181.0   5470.json         true   \n",
       "4         187.0   8320.json  mostly-true   \n",
       "\n",
       "                                       headline_text    subject  \\\n",
       "0  Rebuilding three high schools will benefit 40 ...  education   \n",
       "1  On an income cap for recipients of the popular...  education   \n",
       "2  Says states mandated tests come from an Englis...  education   \n",
       "3  Pasco County schools have graduation rates sub...  education   \n",
       "4  Today many Florida teachers are at risk of hav...  education   \n",
       "\n",
       "              speaker                                  speakerjobtitle  \\\n",
       "0        carole-smith                  Portland schools superintendent   \n",
       "1        jason-carter                                    State Senator   \n",
       "2     george-lavender                             State representative   \n",
       "3  heather-fiorentino  superintendent of Pasco County's public schools   \n",
       "4       charlie-crist                                                0   \n",
       "\n",
       "  stateinfo partyaffiliation  barelytruecounts    ...     mostlytrueocunts  \\\n",
       "0    Oregon             none               0.0    ...                  0.0   \n",
       "1   Georgia         democrat               1.0    ...                  1.0   \n",
       "2     Texas       republican               0.0    ...                  1.0   \n",
       "3   Florida       republican               0.0    ...                  0.0   \n",
       "4   Florida         democrat              15.0    ...                 19.0   \n",
       "\n",
       "   pantsonfirecounts                                   context  \\\n",
       "0                1.0                          a press briefing   \n",
       "1                0.0                                     forum   \n",
       "2                0.0  in Texas House of Representatives debate   \n",
       "3                1.0                           a press release   \n",
       "4                2.0                   a Tampa Bay Times op-ed   \n",
       "\n",
       "                                               clean  \\\n",
       "0  rebuild three high school benefit percent port...   \n",
       "1          incom cap recipe popular hope scholarship   \n",
       "2         say state mandat test come english company   \n",
       "3  pisco county school graduate rate substantia h...   \n",
       "4  today mani florida teacher risk pay impact per...   \n",
       "\n",
       "               sentiment_vector vader_polarity sentiment_score  Unnamed: 0_y  \\\n",
       "0     [0.0, 0.75, 0.25, 0.4588]            0.0          0.4588            80   \n",
       "1   [0.0, 0.412, 0.588, 0.6908]            1.0          0.6908           124   \n",
       "2          [0.0, 1.0, 0.0, 0.0]            0.0          0.0000           168   \n",
       "3          [0.0, 1.0, 0.0, 0.0]            0.0          0.0000           181   \n",
       "4  [0.241, 0.759, 0.0, -0.3612]            0.0         -0.3612           187   \n",
       "\n",
       "   sentiment  polarity  \n",
       "0        0.1         0  \n",
       "1        0.2         0  \n",
       "2        0.0         0  \n",
       "3       -0.3        -1  \n",
       "4       -0.3        -1  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentimentTrain = pd.read_csv('input_data/sentiment_train.csv')\n",
    "sentimentTest = pd.read_csv('input_data/sentiment_test.csv')\n",
    "\n",
    "train = pd.read_csv('input_data/train_sentiment.csv')\n",
    "test = pd.read_csv('input_data/test_sentiment.csv')\n",
    "\n",
    "train_merge = train.merge(sentimentTrain, on = 'clean', how = 'outer')\n",
    "test_merge = test.merge(sentimentTest, on = 'clean', how = 'outer')\n",
    "\n",
    "print(test_merge.shape)\n",
    "test_merge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>clean</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>say anni list polit group support third trimme...</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>decline coal start start natur gas took start ...</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>hillary clinton agre john mccain vote give geo...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>health care reform legis like mandat free sex ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>econom turnaround start end term</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              clean  sentiment  \\\n",
       "0           0  say anni list polit group support third trimme...       -0.7   \n",
       "1           1  decline coal start start natur gas took start ...       -0.4   \n",
       "2           2  hillary clinton agre john mccain vote give geo...        0.2   \n",
       "3           3  health care reform legis like mandat free sex ...        0.2   \n",
       "4           4                   econom turnaround start end term        0.1   \n",
       "\n",
       "   polarity  \n",
       "0        -1  \n",
       "1        -1  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentimentTrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10263, 22)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp =  pd.read_csv('input_data/sentiment/my_train_with_raw.csv')\n",
    "train = pd.read_csv('input_data/train_sentiment.csv')\n",
    "temp = train.merge(temp, how = 'outer')\n",
    "\n",
    "temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avid:1 your:1 horrible_book:1 wasted:1 use_it:1 the_entire:1 money.i:1 i_lit:1 i_read:1 lit:1 i_would:1 relationship:1 read:1 a_&lt;num&gt;:1 reader_and:1 reader:1 suffering:1 fire_one:1 i_had:1 year_old:2 gotten:1 horrible:3 lit_this:1 world...don't:1 my:2 one_star:1 headache_the:1 this_book:5 mom:1 was_horrible:1 friend:1 book_horrible:1 star_i:1 back:1 avid_reader:1 than_one:1 life:1 copy:1 rate_it:1 rate:1 my_mom:1 man:1 book_was:1 half:1 on_fire:1 and_then:1 reading_this:1 so:1 lower:1 i_could:1 &lt;num&gt;_year:2 than:1 time:2 half_of:1 time_spent:1 then:1 book:6 and_picked:1 possible:1 spent:1 old_man:1 up_after:1 one:2 horrible_if:1 one_less:1 part:1 was:2 entire:1 less_copy:1 to_rate:1 my_life:1 about_the:1 your_money.i:1 an_avid:1 if:1 the_relationship:1 use:1 a_headache:1 fire:1 lower_than:1 reading:1 a_friend:1 picked:1 purposes:1 then_got:1 waste_your:1 after_my:1 friend_i:1 old:2 man_and:1 and_i:1 world...don't_waste:1 book_on:1 part_about:1 copy_in:1 book_back:1 book_wasted:1 have_i:1 time_and:1 the_world...don't:1 better:1 if_it:1 star:1 got:1 mom_had:1 read_half:1 waste:1 after:1 i:6 about:1 could_use:1 had_gotten:1 was_possible:1 year:2 it_lower:1 relationship_the:1 wasted_my:1 wish:1 wish_i:1 boy:1 purposes_this:1 got_to:1 the_time:1 it_was:1 back_so:1 suffering_from:1 spent_reading:1 book_up:1 less:1 better_purposes:1 headache:1 possible_to:1 money.i_wish:1 for_better:1 it_suffering:1 the_part:1 gotten_it:1 picked_this:1 entire_time:1 old_boy:1 i_am:1 the_&lt;num&gt;:1 boy_had:1 &lt;num&gt;:2 so_i:1 #label#:negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>to_use:1 shallow:1 found:1 he_castigates:1 cas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>avid:1 your:1 horrible_book:1 wasted:1 use_it:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>book_seriously:1 we:1 days_couldn't:1 me_tell:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mass:1 only:1 he:2 help:1 \"jurisfiction\":1 lik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>save_your:1 class_and:1 his_facts:1 opinions:1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  avid:1 your:1 horrible_book:1 wasted:1 use_it:1 the_entire:1 money.i:1 i_lit:1 i_read:1 lit:1 i_would:1 relationship:1 read:1 a_<num>:1 reader_and:1 reader:1 suffering:1 fire_one:1 i_had:1 year_old:2 gotten:1 horrible:3 lit_this:1 world...don't:1 my:2 one_star:1 headache_the:1 this_book:5 mom:1 was_horrible:1 friend:1 book_horrible:1 star_i:1 back:1 avid_reader:1 than_one:1 life:1 copy:1 rate_it:1 rate:1 my_mom:1 man:1 book_was:1 half:1 on_fire:1 and_then:1 reading_this:1 so:1 lower:1 i_could:1 <num>_year:2 than:1 time:2 half_of:1 time_spent:1 then:1 book:6 and_picked:1 possible:1 spent:1 old_man:1 up_after:1 one:2 horrible_if:1 one_less:1 part:1 was:2 entire:1 less_copy:1 to_rate:1 my_life:1 about_the:1 your_money.i:1 an_avid:1 if:1 the_relationship:1 use:1 a_headache:1 fire:1 lower_than:1 reading:1 a_friend:1 picked:1 purposes:1 then_got:1 waste_your:1 after_my:1 friend_i:1 old:2 man_and:1 and_i:1 world...don't_waste:1 book_on:1 part_about:1 copy_in:1 book_back:1 book_wasted:1 have_i:1 time_and:1 the_world...don't:1 better:1 if_it:1 star:1 got:1 mom_had:1 read_half:1 waste:1 after:1 i:6 about:1 could_use:1 had_gotten:1 was_possible:1 year:2 it_lower:1 relationship_the:1 wasted_my:1 wish:1 wish_i:1 boy:1 purposes_this:1 got_to:1 the_time:1 it_was:1 back_so:1 suffering_from:1 spent_reading:1 book_up:1 less:1 better_purposes:1 headache:1 possible_to:1 money.i_wish:1 for_better:1 it_suffering:1 the_part:1 gotten_it:1 picked_this:1 entire_time:1 old_boy:1 i_am:1 the_<num>:1 boy_had:1 <num>:2 so_i:1 #label#:negative\n",
       "0  to_use:1 shallow:1 found:1 he_castigates:1 cas...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
       "1  avid:1 your:1 horrible_book:1 wasted:1 use_it:...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
       "2  book_seriously:1 we:1 days_couldn't:1 me_tell:...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
       "3  mass:1 only:1 he:2 help:1 \"jurisfiction\":1 lik...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
       "4  save_your:1 class_and:1 his_facts:1 opinions:1...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books =pd.read_csv('input_data/sentiment/books/negative.review')\n",
    "books.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500\n",
      "12500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>For a movie that gets no respect there sure ar...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bizarre horror movie filled with famous faces ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A solid, if unremarkable film. Matthau, as Ein...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It's a strange feeling to sit alone in a theat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You probably all already know this by now, but...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  sentiment\n",
       "0  For a movie that gets no respect there sure ar...          1\n",
       "1  Bizarre horror movie filled with famous faces ...          1\n",
       "2  A solid, if unremarkable film. Matthau, as Ein...          1\n",
       "3  It's a strange feeling to sit alone in a theat...          1\n",
       "4  You probably all already know this by now, but...          1"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "data = []\n",
    "\n",
    "for files in glob.glob('/Users/jonathan/Downloads/aclImdb/train/neg/*.txt'):\n",
    "    data.append(open(files, \"r\").read())\n",
    "\n",
    "print(len(data))\n",
    "neg_df = pd.DataFrame(data)\n",
    "\n",
    "neg_df['sentiment'] = -1\n",
    "\n",
    "neg_df.head()\n",
    "\n",
    "data = []\n",
    "\n",
    "for files in glob.glob('/Users/jonathan/Downloads/aclImdb/train/pos/*.txt'):\n",
    "    data.append(open(files, \"r\").read())\n",
    "\n",
    "print(len(data))\n",
    "pos_df = pd.DataFrame(data)\n",
    "pos_df['sentiment'] = 1\n",
    "pos_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 2)\n"
     ]
    }
   ],
   "source": [
    "pos_df.columns = ['raw_text', 'sentiment']\n",
    "neg_df.columns = ['raw_text', 'sentiment']\n",
    "df = neg_df.merge(pos_df, how = 'outer')\n",
    "print(df.shape)\n",
    "df.head()\n",
    "\n",
    "df.to_csv('input_data/imdb_sentiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Working with one of the best Shakespeare sourc...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Well...tremors I, the original started off in ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ouch! This one was a bit painful to sit throug...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I've seen some crappy movies in my life, but t...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Carriers\" follows the exploits of two guys an...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            raw_text  sentiment\n",
       "0  Working with one of the best Shakespeare sourc...         -1\n",
       "1  Well...tremors I, the original started off in ...         -1\n",
       "2  Ouch! This one was a bit painful to sit throug...         -1\n",
       "3  I've seen some crappy movies in my life, but t...         -1\n",
       "4  \"Carriers\" follows the exploits of two guys an...         -1"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def cleaning(raw_news):\n",
    "    import nltk\n",
    "    \n",
    "    # 1. Remove non-letters/Special Characters and Punctuations\n",
    "    news = re.sub(\"[^a-zA-Z]\", \" \", raw_news)\n",
    "    \n",
    "    # 2. Convert to lower case.\n",
    "    news =  news.lower()\n",
    "    \n",
    "    # 3. Tokenize.\n",
    "    news_words = nltk.word_tokenize( news)\n",
    "    \n",
    "    # 4. Convert the stopwords list to \"set\" data type.\n",
    "    stops = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "    \n",
    "    # 5. Remove stop words. \n",
    "    words = [w for w in  news_words  if not w in stops]\n",
    "    \n",
    "    # 6. Lemmentize \n",
    "    wordnet_lem = [ WordNetLemmatizer().lemmatize(w) for w in words ]\n",
    "    \n",
    "    # 7. Stemming\n",
    "    stems = [nltk.stem.SnowballStemmer('english').stem(w) for w in wordnet_lem ]\n",
    "    \n",
    "    # 8. Join the stemmed words back into one string separated by space, and return the result.\n",
    "    return \" \".join(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
